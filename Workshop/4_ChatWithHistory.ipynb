{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbots with History\n",
    "ChatGPT took the world by storm by exposing a powerful language model with a new interface - chat. There are several components that go into building a chatbot.\n",
    "\n",
    "- The model - you can construct a chatbot from a normal language model or a Chat Model. The important thing to remember is that even if you are using a Chat Model, the API itself is stateless, meaning it won't remember previous interactions - you have to pass them in.\n",
    "- PromptTemplate - this will guide how your chatbot acts. Are they sassy? Helpful? These can be used to give your chatbot some character.\n",
    "- Memory - as mentioned above, the models themselves are stateless. Memory brings some concept of state to the table, allowing it remember previous interactions\n",
    "\n",
    "Chatbots are often very powerful and more differentiated when combined with other sources of data. The same techniques that underpin \"Question Answering Over Docs\" can also be used here to give your chatbot access to that data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"{OpenAiEndPoint}\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's chat on same index document we created in previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from Utilities.cogSearchRetriever import CognitiveSearchRetriever\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        baseUrl = f\"{OpenAiEndPoint}\"\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"{OpenAiEndPoint}\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                    openai_api_base=baseUrl,\n",
    "                    openai_api_version=OpenAiVersion,\n",
    "                    deployment_name=OpenAiChat,\n",
    "                    temperature=temperature,\n",
    "                    openai_api_key=OpenAiKey,\n",
    "                    openai_api_type=\"azure\",\n",
    "                    max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(deployment=OpenAiEmbedding, openai_api_key=OpenAiKey, openai_api_type=\"azure\")\n",
    "        \n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "                openai_api_key=OpenAiApiKey,\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a platform that provides a comprehensive set of analytics experiences. It allows creators to focus on their work without having to manage or understand the underlying infrastructure. Fabric includes various components such as Data Engineering, Data Factory, and Data Science, which cater to different analytical needs. It also includes OneLake, a hierarchical storage system that simplifies data management across organizations. Microsoft Fabric offers a unified and seamless experience for data analytics and machine learning tasks.\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "\n",
    "# We can now create a retriever object, which is neccessary to retrieve documents from the index.\n",
    "retriever = CognitiveSearchRetriever(content_key=\"content\",\n",
    "                                                  service_name=SearchService,\n",
    "                                                  api_key=SearchKey,\n",
    "                                                  index_name=indexName,\n",
    "                                                  topK=topK)\n",
    "\n",
    "## Use the ConversationalRetrievalChain to combine the LLM, retriever, and memory into a single object.\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = qa({\"question\": query})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Microsoft Fabric supports real-time analytics use-cases. It brings together components from Power BI, Azure Synapse, and Azure Data Explorer to provide a unified environment for real-time analytics. This integration allows you to perform real-time analytics on your data and leverage the capabilities of Power BI, Data Engineering, Data Science, and more.\n"
     ]
    }
   ],
   "source": [
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = qa({\"question\": query})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a platform that provides a comprehensive set of analytics experiences. It is designed to simplify the process of data engineering, data science, and data integration for creators. Fabric allows users to focus on their work without having to manage or understand the underlying infrastructure. It includes components such as Data Engineering, Data Factory, and Data Science, which offer industry-leading experiences tailored to specific tasks and personas. Fabric also includes OneLake, a hierarchical storage system that provides a unified storage system for all developers within an organization.\n"
     ]
    }
   ],
   "source": [
    "# In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. \n",
    "# In order to do this, we need to initialize a chain without any memory object.\n",
    "chat_history = []\n",
    "\n",
    "qaNoMemory = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = qaNoMemory({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Microsoft Fabric supports real-time analytics use-cases. It brings together components from Power BI, Azure Synapse, and Azure Data Explorer, including Synapse Real-Time Analytics. With Microsoft Fabric, you can perform real-time analytics on your data.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, answer[\"answer\"])]\n",
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = qaNoMemory({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also try using other Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainType = \"map_reduce\"\n",
    "questionGenerator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "docChain = load_qa_chain(llm, chain_type=chainType)\n",
    "\n",
    "mapRChain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=questionGenerator,\n",
    "    combine_docs_chain=docChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, and data science, allowing users to perform tasks such as data transformation, data integration, and machine learning model deployment. It also provides a single, unified storage system for developers, allowing for easy discovery and data sharing, as well as centralized enforcement of policy and security settings. Microsoft Fabric offers various features such as workspaces, notebooks, pipelines, reports, and lakehouses, and provides context-sensitive help through a Help pane.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = mapRChain({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Microsoft Fabric supports real-time analytics use-cases.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, answer[\"answer\"])]\n",
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = mapRChain({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Does it support real-time analytics use-cases?', 'chat_history': [('What is Microsoft Fabric', 'Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, and data science, allowing users to perform tasks such as data transformation, data integration, and machine learning model deployment. It also provides a single, unified storage system for developers, allowing for easy discovery and data sharing, as well as centralized enforcement of policy and security settings. Microsoft Fabric offers various features such as workspaces, notebooks, pipelines, reports, and lakehouses, and provides context-sensitive help through a Help pane.')], 'answer': 'Yes, Microsoft Fabric supports real-time analytics use-cases.'}\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
