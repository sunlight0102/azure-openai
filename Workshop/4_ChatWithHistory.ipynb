{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbots with History\n",
    "ChatGPT took the world by storm by exposing a powerful language model with a new interface - chat. There are several components that go into building a chatbot.\n",
    "\n",
    "- The model - you can construct a chatbot from a normal language model or a Chat Model. The important thing to remember is that even if you are using a Chat Model, the API itself is stateless, meaning it won't remember previous interactions - you have to pass them in.\n",
    "- PromptTemplate - this will guide how your chatbot acts. Are they sassy? Helpful? These can be used to give your chatbot some character.\n",
    "- Memory - as mentioned above, the models themselves are stateless. Memory brings some concept of state to the table, allowing it remember previous interactions\n",
    "\n",
    "Chatbots are often very powerful and more differentiated when combined with other sources of data. The same techniques that underpin \"Question Answering Over Docs\" can also be used here to give your chatbot access to that data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's chat on same index document we created in previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms.openai import AzureOpenAI, OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from Utilities.cogSearchRetriever import CognitiveSearchRetriever\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "embeddingModelType = \"openai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_key = OpenAiKey\n",
    "    openai.api_version = OpenAiVersion\n",
    "    openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=OpenAiDavinci,\n",
    "            temperature=temperature,\n",
    "            openai_api_key=OpenAiKey,\n",
    "            max_tokens=tokenLength,\n",
    "            batch_size=10, \n",
    "            max_retries=12)\n",
    "\n",
    "    logging.info(\"LLM Setup done\")\n",
    "    embeddings = OpenAIEmbeddings(model=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "elif embeddingModelType == \"openai\":\n",
    "    openai.api_type = \"open_ai\"\n",
    "    openai.api_base = \"https://api.openai.com/v1\"\n",
    "    openai.api_version = '2020-11-07' \n",
    "    openai.api_key = OpenAiApiKey\n",
    "    llm = OpenAI(temperature=temperature,\n",
    "            openai_api_key=OpenAiApiKey,\n",
    "            max_tokens=tokenLength)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in the categories of data engineering, data factory, and data science. It also includes features such as OneLake, workspaces, and a help pane.\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "\n",
    "# We can now create a retriever object, which is neccessary to retrieve documents from the index.\n",
    "retriever = CognitiveSearchRetriever(content_key=\"content\",\n",
    "                                                  service_name=SearchService,\n",
    "                                                  api_key=SearchKey,\n",
    "                                                  index_name=indexName,\n",
    "                                                  topK=topK)\n",
    "\n",
    "## Use the ConversationalRetrievalChain to combine the LLM, retriever, and memory into a single object.\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = qa({\"question\": query})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, Microsoft Fabric supports real-time analytics use-cases. It provides Synapse Real-Time Analytics, which includes a KQL database and KQL Queryset to query data in the Data Explorer database.\n"
     ]
    }
   ],
   "source": [
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = qa({\"question\": query})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in the categories of data engineering, data factory, and data science. It also includes OneLake, a unified storage system for all developers, and a hierarchical organizational structure to simplify management across organizations.\n"
     ]
    }
   ],
   "source": [
    "# In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. \n",
    "# In order to do this, we need to initialize a chain without any memory object.\n",
    "chat_history = []\n",
    "\n",
    "qaNoMemory = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = qaNoMemory({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, Microsoft Fabric supports real-time analytics use-cases. Synapse Real-Time Analytics includes a KQL database and KQL Queryset which allow users to query data and manipulate query results.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, answer[\"answer\"])]\n",
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = qaNoMemory({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also try using other Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainType = \"map_reduce\"\n",
    "questionGenerator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "docChain = load_qa_chain(llm, chain_type=chainType)\n",
    "\n",
    "mapRChain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=questionGenerator,\n",
    "    combine_docs_chain=docChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft Fabric is a comprehensive set of analytics experiences designed to work together seamlessly, including Data Engineering, Data Factory, and Data Science. It includes OneLake, which is hierarchical in nature to simplify management across an organization, and provides context sensitive help in the right rail of a browser.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Microsoft Fabric\"\n",
    "answer = mapRChain({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, Microsoft Fabric supports real-time analytics use-cases.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, answer[\"answer\"])]\n",
    "query = \"Does it support real-time analytics use-cases?\"\n",
    "answer = mapRChain({\"question\": query, \"chat_history\": chat_history})\n",
    "outputAnswer = answer['answer']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
