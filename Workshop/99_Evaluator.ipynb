{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of evaluating a document based on the chunking techniques (RecursiveCharacterSplit, FormRecognizer, TikToken) and using different chunkSize and Overlap during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearchVsRetriever import CognitiveSearchVsRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import Replicate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.document_loaders import PDFMinerLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from Utilities.evaluator import createEvaluatorDataSearchIndex, indexEvaluatorDataSections, createEvaluatorDocumentSearchIndex, indexDocs\n",
    "from Utilities.evaluator import createEvaluatorQaSearchIndex, searchEvaluatorQaData, searchEvaluatorDocument, searchEvaluatorDocumentIndexedData\n",
    "from Utilities.evaluator import searchEvaluatorRunIndex, createEvaluatorRunIndex\n",
    "from Utilities.evaluator import createEvaluatorResultIndex, searchEvaluatorRunIdIndex\n",
    "from IPython.display import display, HTML\n",
    "import uuid\n",
    "import random\n",
    "from langchain.chains import QAGenerationChain\n",
    "import itertools\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import time\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0\n",
    "tokenLength = 1000\n",
    "fileName = \"Fabric Get Started.pdf\"\n",
    "regenerateQa = False\n",
    "reEvaluate = False\n",
    "topK = 3\n",
    "totalQuestions = 5\n",
    "retrieverType = \"SimilaritySearch\"\n",
    "promptStyle = \"Descriptive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Variables\n",
    "evaluatorDocumentIndex = \"evaluatordocument\"\n",
    "evaluatorDataIndexName = \"evaluatordata\"\n",
    "evaluatorQaDataIndexName = \"evaluatorqadata\"\n",
    "evaluatorRunIndexName = \"evaluatorrun\"\n",
    "evaluatorRunResultIndexName = \"evaluatorrunresult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(model=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have document inserted into our index\n",
    "documentResponse = searchEvaluatorDocument(SearchService, SearchKey, evaluatorDocumentIndex, fileName)\n",
    "if documentResponse.get_count() > 0:\n",
    "    for doc in documentResponse:\n",
    "        documentId = doc[\"documentId\"]\n",
    "        break\n",
    "else:\n",
    "    documentId = str(uuid.uuid4())\n",
    "    # Create the Evaluator Document Search Index\n",
    "    createEvaluatorDocumentSearchIndex(SearchService, SearchKey, evaluatorDocumentIndex)\n",
    "    # Insert the document metadata\n",
    "    evaluatorDocument = []\n",
    "    evaluatorDocument.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"documentId\": documentId,\n",
    "            \"documentName\": fileName,\n",
    "            \"sourceFile\": fileName,\n",
    "        })\n",
    "    indexDocs(SearchService, SearchKey, evaluatorDocumentIndex, evaluatorDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process our fileName\n",
    "# TODO : Add support for other file types\n",
    "\n",
    "fabricGetStartedPath = \"Data/PDF/\" + fileName\n",
    "# Load the PDF with Document Loader available from Langchain\n",
    "loader = PDFMinerLoader(fabricGetStartedPath)\n",
    "rawDocs = loader.load()\n",
    "# Set the source \n",
    "for doc in rawDocs:\n",
    "    doc.metadata['source'] = fabricGetStartedPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index evaluatordata already exists\n"
     ]
    }
   ],
   "source": [
    "# Process the document and create the chunked Index with different split methods, chunk sizes and overlaps.\n",
    "# Eventually we will add support for different models\n",
    "# Add more Split Methods\n",
    "splitMethods = [\"RecursiveCharacterTextSplitter\"]\n",
    "model = \"GPT3.5\"\n",
    "chunkSizes = ['500', '1000', '1500', '2000']\n",
    "overlaps = ['0', '50', '100', '150']\n",
    "\n",
    "# Create the Evaluator Data Search Index to store our vector Data\n",
    "createEvaluatorDataSearchIndex(SearchService, SearchKey, evaluatorDataIndexName)\n",
    "for splitMethod in splitMethods:\n",
    "    for chunkSize in chunkSizes:\n",
    "        for overlap in overlaps:\n",
    "            # Check if we already have data inserted into our index\n",
    "            dataResponse = searchEvaluatorDocumentIndexedData(SearchService, SearchKey, evaluatorDataIndexName, documentId, \n",
    "                                                 splitMethod, chunkSize, overlap)\n",
    "            if dataResponse.get_count() == 0:\n",
    "                print(\"Processing Split Method: \" + splitMethod + \" Chunk Size: \" + chunkSize + \" Overlap: \" + overlap)\n",
    "                # Split the document into chunks of 500 characters & 0 overlap\n",
    "                splitter = RecursiveCharacterTextSplitter(chunk_size=chunkSize, chunk_overlap=overlap)\n",
    "                docs = splitter.split_documents(rawDocs)\n",
    "                indexEvaluatorDataSections(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, \n",
    "                            SearchKey, embeddingModelType, fileName, evaluatorDataIndexName, docs, \n",
    "                            splitMethod, chunkSize, overlap, model, embeddingModelType, documentId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEvaluation(data, chunk):\n",
    "    # Generate random starting index in the doc to draw question from\n",
    "    noOfChar = len(data)\n",
    "    startingIndex = random.randint(0, noOfChar-chunk)\n",
    "    subSequence = data[startingIndex:startingIndex+chunk]\n",
    "    # Set up QAGenerationChain chain using GPT 3.5 as default\n",
    "    chain = QAGenerationChain.from_llm(llm)\n",
    "    evalSet = []\n",
    "    # Catch any QA generation errors and re-try until QA pair is generated\n",
    "    awaitingAnswer = True\n",
    "    while awaitingAnswer:\n",
    "        try:\n",
    "            qaPair = chain.run(subSequence)\n",
    "            evalSet.append(qaPair)\n",
    "            awaitingAnswer = False\n",
    "        except JSONDecodeError:\n",
    "            startingIndex = random.randint(0, noOfChar-chunk)\n",
    "            subSequence = data[startingIndex:startingIndex+chunk]\n",
    "    evalPair = list(itertools.chain.from_iterable(evalSet))\n",
    "    return evalPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have indexed the documents, let's go ahead and create the set of the QA pairs for the document and store that in the index\n",
    "# We will use the same QA Pair for evaluating all the different chunk sizes and overlap\n",
    "# Check first if we have already generated the QA pairs for this document\n",
    "# If we have, then we will just use that\n",
    "# If not, then we will generate the QA pairs and store them in the index\n",
    "r = searchEvaluatorQaData(SearchService, SearchKey, evaluatorQaDataIndexName, documentId)\n",
    "evaluatorQaData = []\n",
    "if r.get_count() == 0 or regenerateQa:\n",
    "    generateTotalQuestions = 15\n",
    "    generatedQAPairs = []\n",
    "    for i in range(generateTotalQuestions):\n",
    "        # Generate one question\n",
    "        evalPair = generateEvaluation(rawDocs[0].page_content, 3000)\n",
    "        if len(evalPair) == 0:\n",
    "            # Error in eval generation\n",
    "            continue\n",
    "        else:\n",
    "            # This returns a list, so we unpack to dict\n",
    "            evalPair = evalPair[0]\n",
    "            generatedQAPairs.append(evalPair)\n",
    "    # Create the Evaluator Document Search Index\n",
    "    createEvaluatorQaSearchIndex(SearchService, SearchKey, evaluatorQaDataIndexName)\n",
    "    # Insert the document metadata\n",
    "    if regenerateQa:\n",
    "        i=0\n",
    "        for qa in r:\n",
    "            evaluatorQaData.append({\n",
    "                \"id\": qa['id'],\n",
    "                \"documentId\": qa['documentId'],\n",
    "                \"questionId\": qa['questionId'],\n",
    "                \"question\": generatedQAPairs[i]['question'],\n",
    "                \"answer\": generatedQAPairs[i]['answer'],\n",
    "            })\n",
    "            i+=1\n",
    "    else:\n",
    "        for qa in generatedQAPairs:\n",
    "            evaluatorQaData.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"documentId\": documentId,\n",
    "                \"questionId\": str(uuid.uuid4()),\n",
    "                \"question\": qa['question'],\n",
    "                \"answer\": qa['answer'],\n",
    "            })\n",
    "    indexDocs(SearchService, SearchKey, evaluatorQaDataIndexName, evaluatorQaData)\n",
    "else:\n",
    "    for qa in r:\n",
    "            evaluatorQaData.append({\n",
    "                \"id\": qa['id'],\n",
    "                \"documentId\": qa['documentId'],\n",
    "                \"questionId\": qa['questionId'],\n",
    "                \"question\": qa['question'],\n",
    "                \"answer\": qa['answer'],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QaChainPrompt = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "\n",
    "promptStyleFast = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "You are also asked to identify potential sources of bias in the question and in the true answer.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should be as follows:\n",
    "\n",
    "GRADE: (Correct or Incorrect)\n",
    "(line break)\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect, identify potential sources of bias in the QUESTION, and identify potential sources of bias in the TRUE ANSWER. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "\"\"\"\n",
    "\n",
    "promptStyleBias = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are assessing a submitted student answer to a question relative to the true answer based on the provided criteria: \n",
    "    \n",
    "    ***\n",
    "    QUESTION: {query}\n",
    "    ***\n",
    "    STUDENT ANSWER: {result}\n",
    "    ***\n",
    "    TRUE ANSWER: {answer}\n",
    "    ***\n",
    "    Criteria: \n",
    "      relevance:  Is the submission referring to a real quote from the text?\"\n",
    "      conciseness:  Is the answer concise and to the point?\"\n",
    "      correct: Is the answer correct?\"\n",
    "    ***\n",
    "    Does the submission meet the criterion? First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print \"Correct\" or \"Incorrect\" (without quotes or punctuation) on its own line corresponding to the correct answer.\n",
    "    Reasoning:\n",
    "\"\"\"\n",
    "\n",
    "promptStyleGrading = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should be as follows:\n",
    "\n",
    "GRADE: (Correct or Incorrect)\n",
    "(line break)\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "\"\"\"\n",
    "\n",
    "promptStyleDefault = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {query}\n",
    "    Here are some documents retrieved in response to the question: \\n\n",
    "    {result}\n",
    "    And here is the answer to the question: \\n \n",
    "    {answer}\n",
    "    Criteria: \n",
    "      relevance: Are the retrieved documents relevant to the question and do they support the answer?\"\n",
    "    Do the retrieved documents meet the criterion? Print \"Correct\" (without quotes or punctuation) if the retrieved context are relevant or \"Incorrect\" if not (without quotes or punctuation) on its own line. \"\"\"\n",
    "\n",
    "gradeDocsPromptFast = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" \n",
    "    Given the question: \\n\n",
    "    {query}\n",
    "    Here are some documents retrieved in response to the question: \\n\n",
    "    {result}\n",
    "    And here is the answer to the question: \\n \n",
    "    {answer}\n",
    "    Criteria: \n",
    "      relevance: Are the retrieved documents relevant to the question and do they support the answer?\"\n",
    "\n",
    "    Your response should be as follows:\n",
    "\n",
    "    GRADE: (Correct or Incorrect, depending if the retrieved documents meet the criterion)\n",
    "    (line break)\n",
    "    JUSTIFICATION: (Write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "    \"\"\"\n",
    "\n",
    "gradeDocsPromptDefault = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradeModelAnswer(predictedDataSet, predictions, promptStyle):\n",
    "    if promptStyle == \"Fast\":\n",
    "        prompt = promptStyleFast\n",
    "    elif promptStyle == \"Descriptive w/ bias check\":\n",
    "        prompt = promptStyleBias\n",
    "    elif promptStyle == \"OpenAI grading prompt\":\n",
    "        prompt = promptStyleGrading\n",
    "    else:\n",
    "        prompt = promptStyleDefault\n",
    "\n",
    "    # Note: GPT-4 grader is advised by OAI \n",
    "    evalChain = QAEvalChain.from_llm(llm=llm,\n",
    "                                      prompt=prompt)\n",
    "    gradedOutputs = evalChain.evaluate(predictedDataSet,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return gradedOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradeModelRetrieval(getDataSet, predictions, gradeDocsPrompt):\n",
    "    if gradeDocsPrompt == \"Fast\":\n",
    "        prompt = gradeDocsPromptFast\n",
    "    else:\n",
    "        prompt = gradeDocsPromptDefault\n",
    "\n",
    "    # Note: GPT-4 grader is advised by OAI\n",
    "    evalChain = QAEvalChain.from_llm(llm=llm,prompt=prompt)\n",
    "    gradedOutputs = evalChain.evaluate(getDataSet,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return gradedOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEvaluation(chain, retriever, evalQaPair, gradePrompt):\n",
    "    predictions = []\n",
    "    retrievedDocs = []\n",
    "    gtDataSet = []\n",
    "    latency = []\n",
    "\n",
    "    print(\"Inside runEvaluation\")\n",
    "    print(evalQaPair)\n",
    "    # Get answer and log latency\n",
    "    try:\n",
    "        startTime = time.time()\n",
    "        predictions.append(chain({\"query\": evalQaPair[\"question\"]}, return_only_outputs=True))\n",
    "        gtDataSet.append(evalQaPair)\n",
    "        endTime = time.time()\n",
    "        elapsedTime = endTime - startTime\n",
    "        latency.append(elapsedTime)\n",
    "    except:\n",
    "        predictions.append({'result': 'Error in prediction'})\n",
    "        print(\"Error in prediction\")\n",
    "\n",
    "    # Extract text from retrieved docs\n",
    "    retrievedDocText = \"\"\n",
    "    docs = retriever.get_relevant_documents(evalQaPair[\"question\"])\n",
    "    for i, doc in enumerate(docs):\n",
    "        retrievedDocText += \"Doc %s: \" % str(i+1) + \\\n",
    "            doc.page_content + \" \"\n",
    "\n",
    "    # Log\n",
    "    retrieved = {\"question\": evalQaPair[\"question\"],\n",
    "                 \"answer\": evalQaPair[\"answer\"], \"result\": retrievedDocText}\n",
    "    retrievedDocs.append(retrieved)\n",
    "\n",
    "    # Grade\n",
    "    gradeAnswers = gradeModelAnswer(gtDataSet, predictions, gradePrompt)\n",
    "    gradeRetrieval = gradeModelRetrieval(gtDataSet, retrievedDocs, gradePrompt)\n",
    "    return gradeAnswers, gradeRetrieval, latency, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEvaluator(totalQuestions, chain, retriever, promptStyle, testDataSet):\n",
    "    d = pd.DataFrame(columns=['question', 'answer', 'predictedAnswer', 'answerScore', 'retrievalScore', 'latency'])\n",
    "    for i in range(totalQuestions):\n",
    "        # gradedAnswer, gradedRetrieval, latency, predictions = runEvaluation(\n",
    "        #         chain, retriever, evaluatorQaData[i], promptStyle)\n",
    "\n",
    "        predictions = []\n",
    "        retrievedDocs = []\n",
    "        gtDataSet = []\n",
    "        latency = []\n",
    "        currentDataSet = testDataSet[i]\n",
    "    \n",
    "        try:\n",
    "            startTime = time.time()\n",
    "            predictions.append(chain({\"query\": currentDataSet[\"question\"]}, return_only_outputs=True))\n",
    "            gtDataSet.append(currentDataSet)\n",
    "            endTime = time.time()\n",
    "            elapsedTime = endTime - startTime\n",
    "            latency.append(elapsedTime)\n",
    "        except:\n",
    "            predictions.append({'result': 'Error in prediction'})\n",
    "            print(\"Error in prediction\")\n",
    "\n",
    "        # Extract text from retrieved docs\n",
    "        retrievedDocText = \"\"\n",
    "        docs = retriever.get_relevant_documents(currentDataSet[\"question\"])\n",
    "        for i, doc in enumerate(docs):\n",
    "            retrievedDocText += \"Doc %s: \" % str(i+1) + \\\n",
    "                doc.page_content + \" \"\n",
    "\n",
    "        # Log\n",
    "        retrieved = {\"question\": currentDataSet[\"question\"],\n",
    "                    \"answer\": currentDataSet[\"answer\"], \"result\": retrievedDocText}\n",
    "        retrievedDocs.append(retrieved)\n",
    "\n",
    "        # Grade\n",
    "        gradedAnswer = gradeModelAnswer(gtDataSet, predictions, promptStyle)\n",
    "        gradedRetrieval = gradeModelRetrieval(gtDataSet, retrievedDocs, promptStyle)\n",
    "\n",
    "        # Assemble output\n",
    "        # Summary statistics\n",
    "        dfOutput = {'question': evaluatorQaData[i]['question'], 'answer': evaluatorQaData[i]['answer'],\n",
    "                    'predictedAnswer': predictions[0]['result'], 'answerScore': [{'score': 1 if \"Incorrect\" not in text else 0,\n",
    "                                'justification': text} for text in [g['text'] for g in gradedAnswer]], \n",
    "                                'retrievalScore': [{'score': 1 if \"Incorrect\" not in text else 0,\n",
    "                                'justification': text} for text in [g['text'] for g in gradedRetrieval]],\n",
    "                    'latency': latency}\n",
    "\n",
    "        # Add to dataframe\n",
    "        d = pd.concat([d, pd.DataFrame(dfOutput)], axis=0)\n",
    "\n",
    "        # Convert dataframe to dict\n",
    "    d_dict = d.to_dict('records')\n",
    "    return d_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluatorrunresult search index\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 500 0\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 500 50\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 500 100\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 500 150\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1000 0\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1000 50\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1000 100\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1000 150\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1500 0\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1500 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='fpdoaoai.openai.azure.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1500 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='fpdoaoai.openai.azure.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 1500 150\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 2000 0\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 2000 50\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 2000 100\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n",
      "Processing:  cba884c5-8922-47b0-9ac9-da60aaa4041e SimilaritySearch Descriptive RecursiveCharacterTextSplitter 2000 150\n",
      "Total docs: 5\n",
      "\tIndexed 5 sections, 5 succeeded\n"
     ]
    }
   ],
   "source": [
    "# Select retriever\n",
    "finalOutput = []\n",
    "#chunkSizes = ['1500']\n",
    "#overlaps = ['150']\n",
    "\n",
    "#createEvaluatorRunIndex(SearchService, SearchKey, evaluatorRunIndexName)\n",
    "createEvaluatorResultIndex(SearchService, SearchKey, evaluatorRunResultIndexName)\n",
    "# Check if we already have runId for this document\n",
    "r = searchEvaluatorRunIdIndex(SearchService, SearchKey, evaluatorRunResultIndexName, documentId)\n",
    "if r.get_count() == 0:\n",
    "    runId = str(uuid.uuid4())\n",
    "else:\n",
    "    for run in r:\n",
    "        runId = run['runId']\n",
    "        break\n",
    "for splitMethod in splitMethods:\n",
    "    for chunkSize in chunkSizes:\n",
    "        for overlap in overlaps:\n",
    "            # Verify if we have created the Run ID\n",
    "            r = searchEvaluatorRunIndex(SearchService, SearchKey, evaluatorRunResultIndexName, documentId, retrieverType, \n",
    "                                    promptStyle, splitMethod, chunkSize, overlap)\n",
    "            if r.get_count() == 0 or reEvaluate:\n",
    "                # Create the Run ID\n",
    "                print(\"Processing: \", documentId, retrieverType, promptStyle, splitMethod, chunkSize, overlap)\n",
    "                runIdData = []\n",
    "                subRunId = str(uuid.uuid4())\n",
    "                # runIdData.append({\n",
    "                #         \"id\": str(uuid.uuid4()),\n",
    "                #         \"runId\": runId,\n",
    "                #         \"subRunId\": subRunId,\n",
    "                #         \"documentId\": documentId,\n",
    "                #         \"retrieverType\": retrieverType,\n",
    "                #         \"promptStyle\": promptStyle,\n",
    "                #         \"splitMethod\": splitMethod,\n",
    "                #         \"chunkSize\": chunkSize,\n",
    "                #         \"overlap\": overlap,\n",
    "                #     })\n",
    "                # indexDocs(SearchService, SearchKey, evaluatorRunIndexName, runIdData)\n",
    "                \n",
    "                retriever = CognitiveSearchVsRetriever(contentKey=\"contentVector\",\n",
    "                            serviceName=SearchService,\n",
    "                            apiKey=SearchKey,\n",
    "                            indexName=evaluatorDataIndexName,\n",
    "                            topK=topK,\n",
    "                            splitMethod = splitMethod,\n",
    "                            model = model,\n",
    "                            chunkSize = chunkSize,\n",
    "                            overlap = overlap,\n",
    "                            openAiService = OpenAiService,\n",
    "                            openAiKey = OpenAiKey,\n",
    "                            openAiVersion = OpenAiVersion,\n",
    "                            openAiApiKey = OpenAiApiKey,\n",
    "                            documentId = documentId,\n",
    "                            returnFields=[\"id\", \"content\", \"sourceFile\", \"splitMethod\", \"chunkSize\", \"overlap\", \"model\", \"modelType\", \"documentId\"]\n",
    "                            )\n",
    "                vectorStoreChain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, \n",
    "                                                chain_type_kwargs={\"prompt\": QaChainPrompt})\n",
    "                runEvaluations = runEvaluator(totalQuestions, vectorStoreChain, retriever, promptStyle, evaluatorQaData)\n",
    "                runEvaluationData = []\n",
    "                for runEvaluation in runEvaluations:\n",
    "                        runEvaluationData.append({\n",
    "                            \"id\": str(uuid.uuid4()),\n",
    "                            \"runId\": runId,\n",
    "                            \"subRunId\": subRunId,\n",
    "                            \"documentId\": documentId,\n",
    "                            \"retrieverType\": retrieverType,\n",
    "                            \"promptStyle\": promptStyle,\n",
    "                            \"splitMethod\": splitMethod,\n",
    "                            \"chunkSize\": chunkSize,\n",
    "                            \"overlap\": overlap,\n",
    "                            \"question\": runEvaluation['question'],\n",
    "                            \"answer\": runEvaluation['answer'],\n",
    "                            \"predictedAnswer\": runEvaluation['predictedAnswer'],\n",
    "                            \"answerScore\": json.dumps(runEvaluation['answerScore']),\n",
    "                            \"retrievalScore\": json.dumps(runEvaluation['retrievalScore']),\n",
    "                            \"latency\": str(runEvaluation['latency']),\n",
    "                        })\n",
    "                indexDocs(SearchService, SearchKey, evaluatorRunResultIndexName, runEvaluationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is Microsoft Fabric\"\n",
    "# #answer = retriever.get_relevant_documents(question)\n",
    "# answer = vectorStoreChain({\"query\": question}, return_only_outputs=True)\n",
    "# answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
