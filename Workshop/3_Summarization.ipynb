{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "A common use case is wanting to summarize long documents. This naturally runs into the context window limitations. Unlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything). So what do you do then?\n",
    "\n",
    "The most common way around this is to split the documents into chunks and then do summarization in a recursive manner. By this we mean you first summarize each chunk by itself, then you group the summaries into chunks and summarize each chunk of summaries, and continue doing that until only one is left."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"{OpenAiEndPoint}\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the document/PDF (instead of getting that data from Vector store or document reposit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# Import required libraries\n",
    "from langchain.llms.openai import AzureOpenAI, OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PDFMinerLoader,\n",
    "    UnstructuredFileLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file name and the namespace for the index\n",
    "fileName = \"Fabric Get Started.pdf\"\n",
    "fabricGetStartedPath = \"Data/PDF/\" + fileName\n",
    "# Load the PDF with Document Loader available from Langchain\n",
    "loader = PDFMinerLoader(fabricGetStartedPath)\n",
    "rawDocs = loader.load()\n",
    "# Set the source \n",
    "for doc in rawDocs:\n",
    "    doc.metadata['source'] = fabricGetStartedPath\n",
    "\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = textSplitter.split_documents(rawDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents chunks generated from PDF :  58\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents chunks generated from PDF : \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Flexibility to change the call to OpenAI or Azure OpenAI\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"{OpenAiEndPoint}\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(deployment=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4096 tokens. However, your messages resulted in 17332 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m chainType \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m summaryChain \u001b[39m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[39m=\u001b[39mchainType)\n\u001b[1;32m----> 4\u001b[0m summary \u001b[39m=\u001b[39m summaryChain\u001b[39m.\u001b[39;49mrun(docs)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSummary: \u001b[39m\u001b[39m\"\u001b[39m, summary)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mother_keys\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:87\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     86\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs), {}\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    199\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chains\\llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m     80\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\base.py:143\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    138\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m    139\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    140\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    142\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\base.py:91\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     90\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     92\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[0;32m     93\u001b[0m generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\base.py:83\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[0;32m     79\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[0;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m     84\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m     85\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     86\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m     87\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[0;32m     88\u001b[0m     ]\n\u001b[0;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     90\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\base.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[0;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m---> 84\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(m, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     85\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     86\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m     87\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[0;32m     88\u001b[0m     ]\n\u001b[0;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     90\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\openai.py:320\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[0;32m    316\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    317\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion, \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: role}\n\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    319\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[1;32m--> 320\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_with_retry(messages\u001b[39m=\u001b[39mmessage_dicts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\openai.py:281\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\langchain\\chat_models\\openai.py:279\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32md:\\Utilities\\Python39\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4096 tokens. However, your messages resulted in 17332 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "# Because we have a large document, it is expected that we will run into Token limitation error with code below\n",
    "chainType = \"stuff\"\n",
    "summaryChain = load_summarize_chain(llm, chain_type=chainType)\n",
    "summary = summaryChain.run(docs)\n",
    "print(\"Summary: \", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a unified platform that offers data and analytics solutions for organizations. It simplifies analytics needs by providing an integrated and easy-to-use product. It combines components from Power BI, Azure Synapse, and Azure Data Explorer into a unified environment. Fabric allows users to focus on their work without needing to understand or manage the underlying infrastructure. It also offers a Data Warehouse experience and real-time analytics. Power BI is highlighted as the leading business intelligence platform in Fabric. OneLake is a unified storage system that simplifies data management and sharing. The Fabric (Preview) trial allows users to store workspaces and items and run Fabric experiences. It provides access to all Fabric experiences and features, as well as 1 TB of OneLake storage. The trial capacity can be shared with others or additional capacity can be purchased. The Help pane and Fabric settings pane provide assistance and personalization options. Workspaces in Fabric are collaborative spaces for organizing items, and sensitivity labels can be applied to protect sensitive content.\n"
     ]
    }
   ],
   "source": [
    "# Let's change now the chaintype from stuff to mapreduce and refine to see the summary\n",
    "chainType = \"map_reduce\"\n",
    "summaryChain = load_summarize_chain(llm, chain_type=chainType, return_intermediate_steps=True)\n",
    "summary = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "outputAnswer = summary['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a unified platform that provides data and analytics solutions for organizations. It offers a comprehensive suite of services, including data lake, data engineering, and data integration. Fabric simplifies analytics needs by providing an integrated and easy-to-use product. It is built on a foundation of Software as a Service (SaaS)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a SaaS platform that integrates various components from Power BI, Azure Synapse, and Azure Data Explorer into a unified environment. It offers a range of analytics capabilities, shared experiences, easy access to assets, a unified data lake, centralized administration, and automatic data sensitivity labels. It is currently in preview and subject to potential modifications before release."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences, tailored to specific personas and tasks. It includes industry-leading experiences in data engineering, data factory, and data science. These experiences provide tools for large-scale data transformation, data integration, and machine learning model building and deployment. Fabric allows creators to focus on their work without needing to understand or manage the underlying infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This summary provides an overview of the features and capabilities of Microsoft Fabric. It mentions that data scientists can use the platform to enrich organizational data with predictions and integrate them into BI reports, shifting from descriptive to predictive insights. The platform also offers a Data Warehouse experience with industry-leading SQL performance and scale, as well as real-time analytics for observational data. Additionally, Power BI is highlighted as the leading business intelligence platform that allows quick and intuitive access to data in Fabric for better decision-making."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a comprehensive big data analytics platform that combines the OneLake and lakehouse architectures. OneLake, also known as Microsoft Fabric Lake, is built on Azure Data Lake Storage Gen2 and provides a unified location for storing organizational data. It simplifies the user experience by eliminating the need to understand infrastructure concepts and does not require an Azure account. OneLake aims to eliminate data silos and provide actionable insights from large and complex data repositories."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> OneLake is a unified storage system that simplifies data management and sharing for developers. It is hierarchical in structure and allows for easy creation of workspaces and lakehouses. It is built into Microsoft Fabric and does not require upfront provisioning. Each tenant has one OneLake and data is divided into manageable containers."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> OneLake is a data storage solution that allows users to ingest, process, analyze, and collaborate on data, similar to OneDrive in Office. Microsoft Fabric compute experiences are integrated with OneLake, and various data-related applications use OneLake as their native store. OneLake allows for instant mounting of existing storage accounts and enables easy data sharing without duplication. The shortcut feature also extends to other storage systems, facilitating data composition and analysis across clouds. The Microsoft Fabric (Preview) trial provides access to the Fabric product experiences."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to start a Fabric (Preview) trial, which is currently in the pre-release stage. Existing Power BI users can skip to starting the trial, while new users need to sign up for a Power BI free license before starting the trial. The trial lasts until the general availability (GA) version is released and is extended for an additional 60 days. The steps to start the trial include accessing the Fabric homepage, selecting the Account manager, agreeing to the terms, and receiving a confirmation message."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The summary is about the Fabric (Preview) trial offered by Microsoft. The trial includes a Power BI individual trial and a Fabric trial capacity. Users can start the trial by creating a Fabric item in a workspace that doesn't support Fabric items. A trial capacity is a pool of resources allocated to Microsoft Fabric, and the size of the capacity determines the computation power available. With the trial, users have access to all Fabric experiences and features, as well as 1 TB of OneLake storage. They can create workspaces and collaborate with others in the same trial capacity."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Fabric allows users to share and collaborate on datasets, warehouses, and notebooks. Users can create analytics solutions using these items. To access capacity, users need to add items to their workspace and assign the workspace to their trial capacity. Microsoft provides a 64 capacity unit trial capacity, which allows users to consume 64x60 CU seconds per minute. The consumption rate varies for different functions. There is no limit on the number of workspaces or items that can be created, but the availability of capacity units and the consumption rate determine the usage."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The summary of the given text is that as the capacity owner and administrator for a Fabric trial capacity, you have access to a detailed report on how capacity units are consumed. If you cancel your trial, all workspaces and their contents will be deleted, and you will lose the ability to create workspaces, share Fabric items, and create analytics solutions. Power BI administrators can enable or disable trials for paid features for both Power BI and Fabric, but there can only be one capacity administrator per trial capacity."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Administrators currently cannot view metrics for individual capacities in Power BI, but there are plans to add this feature in the future. If the \"Start trial\" button is not visible in the Account manager, it may be disabled by the Power BI administrator. Existing trial users can start a Fabric (Preview) trial by attempting to create a Fabric item. If the trial capacity limit has been reached, options include purchasing a Fabric capacity from Azure, requesting another trial capacity user to share their capacity, or contacting the Power BI administrator to increase the tenant trial capacity limits. Assigning a workspace to the trial capacity is not possible in Workplace settings."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This bug occurs when the Power BI administrator turns off trials after it has already been started. To add your workspace to the trial capacity, go to the Admin portal and select Trial > Capacity settings. If your home region doesn't have Fabric enabled, you won't be able to create any Fabric items in your trial capacity. The Fabric (Preview) trial is different from an individual trial of Power BI paid as it allows access to the Fabric landing page."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Fabric (Preview) trial allows users to store Fabric workspaces and items and run Fabric experiences. It requires a Fabric capacity to access non-Power BI experiences and items. Private links and private access may limit the creation of Fabric items during the trial. Autoscale is not supported in the trial capacity, but users can purchase a Fabric capacity in Azure for more compute capacity. The Fabric (Preview) trial is different from a Proof of Concept (POC) and does not require customization or financial investment. Users can sign up for a free trial and start using product experiences immediately. An Azure subscription is not necessary for the trial, but existing Power BI users can purchase a paid Fabric capacity if they have an existing Azure subscription."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides information on how to migrate existing workspaces into a trial capacity using workspace settings. It also explains the terminology used in Microsoft Fabric, including terms specific to Synapse Data Warehouse, Synapse Data Engineering, Synapse Data Science, Synapse Real-Time Analytics, Data Factory, and Power BI. The article emphasizes that Microsoft Fabric is currently in preview and may undergo substantial modifications before its release."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This passage provides an overview of different items and concepts within the Synapse Data Engineering experience. It mentions the capabilities of the Data Engineering experience, including the lakehouse, notebook, and Spark job definition items. It also defines the terms \"tenant\" and \"workspace\" and explains their roles in the Fabric environment. Additionally, it describes the lakehouse as a collection of files, folders, and tables used for big data processing, and the notebook as a multi-language interactive programming tool."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This passage discusses Spark applications and jobs, as well as the concept of a Spark job definition. It also mentions V-order, a write optimization for the parquet file format. Additionally, it briefly mentions Data Factory and its connectors."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This summary provides an overview of different data-related tools and features. It mentions connectors for connecting to different data stores, data pipelines for orchestrating data movement and transformation, Dataflow Gen2 for ingesting and transforming data from various sources, Data Wrangler as a notebook-based tool for exploratory data analysis, and machine learning experiments as a unit of organization and control for machine learning runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This summary provides an overview of different terms related to machine learning and data analytics. It mentions the concept of a machine learning model, which is a trained file used to recognize patterns. It also discusses the concept of a run, which refers to the execution of model code. The summary then introduces the Synapse data warehousing functionality, which includes a SQL Endpoint for querying data and a traditional data warehouse for transactional capabilities. Lastly, it mentions the KQL database and KQL Queryset, which are used for executing queries and manipulating query results in the context of real-time analytics."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Microsoft Fabric platform offers event streams for capturing, transforming, and routing real-time events. OneLake shortcuts allow users to connect to existing data without copying it. Microsoft Fabric provides end-to-end tutorials to guide users through the data acquisition and consumption process. Microsoft Fabric is currently in the preview stage."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides a list of tutorials for different experiences within Microsoft's Fabric platform. The tutorials cover various scenarios such as ingesting and analyzing data, building machine learning models, and creating reports. The document also mentions that the product may undergo significant modifications before its release and that Microsoft does not provide any warranties for the information provided."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This summary provides an overview of the tutorials and resources available for data integration, data science, and price prediction using Microsoft's Data Factory and Fabric. It explains how to ingest and transform data, create AI models, and solve common business problems. It also mentions the availability of a decision guide for choosing between copy activity, dataflow, and Spark for different workloads."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The text discusses various aspects of dataflow generation, including data ingestion, transformation, wrangling, and profiling. It mentions different roles involved in data engineering, integration, and analysis. The use of ETL, SQL, and Spark is highlighted, along with the options of no code and low code development. The text also mentions the volume of data, development interfaces, connectors, and transformation complexity. It concludes by presenting a scenario where a data engineer needs to ingest a large volume of data from external systems."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Leo wants to process data using Spark with minimal coding and prefers a drag and drop UI. He needs to get raw data from various sources into a consolidated lakehouse. He selects pipeline copy activity to copy the data, both historical and incremental, to the lakehouse. Mary, on the other hand, is responsible for cleaning and applying business logic to the data and loading it into multiple destinations for reporting teams."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Mary, an experienced Power Query user, decides to use Dataflow Gen 2 for data transformation. Adam, a data engineer at a retail company, chooses to use Spark to build extract and transformation logic for customer review analytics. He writes a Spark application to read, cleanse, transform, and write data to Delta tables in the company's lakehouse."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides a decision guide for choosing between a data warehouse and a lakehouse for workloads using Microsoft Fabric. It outlines the properties of both options, including data volume and type, developer skill sets, data organization, operations, and development interfaces. The article also mentions that Microsoft Fabric is currently in preview and may be subject to substantial modifications before its release."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The summary states that Susan, a professional developer, is new to Microsoft Fabric and needs to decide whether to build a data warehouse or a lakehouse. After reviewing the available options and considering the skill set and need for multi-table transactions, Susan decides to use a data warehouse. This decision is based on her familiarity with SQL syntax and functionality, as well as the fact that the primary consumers of the data are also skilled with SQL and SQL analytical tools."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> In scenario 2, a data engineer named Rob needs to store and model large amounts of data. He decides to use a lakehouse solution to leverage the diverse skills of his team, including those who are skilled in T-SQL.\n",
       "\n",
       "In scenario 3, a Power BI developer named Ash needs to build a data product for a business unit. They determine that a data warehouse or lakehouse is not necessary for their needs, and instead opt for a self-service, no-code solution with data volumes under 100 GB. They work with business analysts who are familiar with Power BI and Microsoft Office."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides an overview of navigating to items and actions from Microsoft Fabric Home. It explains that each product experience has its own Home and describes the similarities between them. The article also mentions that Microsoft Fabric is currently in preview and may be subject to substantial modifications before its release. It further discusses the items that can be seen on Home, which are created by the user and accessible within their workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The summary states that Microsoft Fabric allows users to create and share various items such as apps, reports, and more. The Home page in Microsoft Fabric is not specific to a workspace and can include items from multiple workspaces. Users can navigate to a specific workspace using the navigation pane and workspace selector. The Home page displays the items that users can access, and if it becomes crowded, users can use the global search function to quickly find what they need. The layout and content on the Home page vary for each user and product experience. The left navigation pane provides links to different views of items and creator resources, and there is a selector for switching between product experiences."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The top menu bar in Microsoft Fabric allows users to navigate, find items, access help, and provide feedback. The Account manager control is important for managing account information and Fabric trials. Users can create new items and access recommended content. The Home page organizes items by recent, favorites, and items shared by colleagues. Users can locate and view content through searching, using the navigation pane, or selecting a card on the Home canvas."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The nav pane in Microsoft Fabric's Data factory is used to organize actions and quickly access items. The bottom section of the nav pane displays workspaces, which can be opened using the workspace selector. Workspaces are collaborative spaces for creating collections of items. The nav pane remains visible throughout different areas of Microsoft Fabric and includes options such as Home, Browse, OneLake data hub, Create, and Workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The nav pane in Microsoft Fabric allows users to view the content of a workspace, including notebooks, pipelines, reports, and lakehouses. The experience selector in the bottom left corner allows users to open and make active other product experiences. Users can search, sort, and filter their content using various methods. The Help pane provides context-sensitive help and allows users to search for answers to questions. It displays articles related to the current view and community posts if available. The Help pane can be left open for easy access or closed to save screen space."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Help pane is a useful tool for finding answers to questions. It allows users to search for information and provides self-help resources. If the self-help answers are not sufficient, there are additional resources available at the bottom of the Help pane. The Account manager provides information about the user's account and license. The Home page also includes icons for notifications, settings, and feedback. The center area of the Home page is called the canvas and contains the content that the user needs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides an overview of the Power BI Home canvas, which displays options for creating new items, recommended items, recents, favorites, and shared content. It also explains how to use the Fabric Help pane, which provides articles and search functionality for the Fabric documentation and community forums. The article notes that Microsoft Fabric is currently in preview and may be subject to substantial modifications before its release."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Help pane in Fabric is the default view that shows recommended topics and resources based on the current context and location. It has three sections: feature-aware documents, forum topics, and other resources. The Help pane also functions as a search engine, allowing users to enter keywords to find relevant information. It is a useful tool for learning and getting started with Fabric."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Help pane in Microsoft Fabric provides articles to help users learn about the data hub. Users can open the Help pane, browse recent features, search for specific topics, and access community forums for additional assistance. The Help pane can be closed by selecting the X icon. If further help is needed, users can ask the community, submit an idea, or access the Support site. Additionally, the Global search feature allows users to search, filter, and sort through their content in Microsoft Fabric."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information about using the search function in Microsoft Fabric to find items, content, and colleagues. It explains how to use the search box and filter by keyword feature to narrow down search results."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The article discusses how to sort and filter content lists in Microsoft Fabric. Sorting helps users find items in long lists, and can be done by selecting the arrow next to the name. Filtering is another way to quickly locate content, and can be accessed by selecting \"Filter\" in the upper right corner. The article also mentions that Microsoft Fabric is currently in preview and may be subject to changes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article explains how to access and navigate the Fabric settings pane, which provides various settings options. Users can configure preferences, language, notifications, item settings, developer settings, and access resources and extensions. The Fabric settings pane allows users to personalize their experience and manage their account and data."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This summary provides an overview of the different features and functionalities available in Power BI, including managing connections and gateways, managing embed codes, migrating Azure Analysis Services datasets to Power BI Premium, and accessing governance and insights settings. It also mentions the availability of the Microsoft Purview hub for Power BI admins to view insights about sensitive data and access governance and compliance capabilities. Workspaces are highlighted as collaborative spaces for creating collections of items."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides information on how to work with workspaces, including tips on pinning favorite workspaces, managing access with different roles, and navigating to the current workspace. It also explains how to update and manage workspace configurations, set up a contact list for notifications, and access workspace settings for general and specific settings."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides instructions on how to access workspace settings in Microsoft Fabric. It also explains the contact list feature, which allows users to receive notifications about workspace issues. Additionally, it discusses the Workspace OneDrive feature, which allows users to configure a Microsoft 365 Group for accessing SharePoint document libraries. It advises users to consult their IT department if they encounter any restrictions or limitations. Lastly, it mentions that Microsoft Fabric does not synchronize permissions between workspace access and Microsoft 365 Group membership."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This passage discusses how to configure storage and manage workspace access in Microsoft 365. It explains how to configure OneDrive and manage membership of the Microsoft 365 Group. It also mentions the option to assign workspaces to different capacities within the organization. Additionally, it discusses the configuration of Azure Data Lake Gen 2 storage and Azure Log Analytics connection for dataflow storage and performance logs. Finally, it mentions the use of system storage for managing dataset storage."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on managing workspace accounts, including storage usage and deleting items. It also explains how to remove a workspace and the administrative and auditing capabilities for workspaces. The document mentions that deleting a dataset may affect reports and dashboards. Additionally, it mentions that Microsoft Fabric audits activities related to workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides instructions on how to create workspaces in Microsoft Fabric. Workspaces are used to organize collections of items such as lakehouses, warehouses, and reports. The article explains the steps to create a workspace, including giving it a unique name, providing a description, and assigning it to a domain. The article also mentions limitations, such as the maximum number of datasets and reports allowed in a workspace, and certain special characters not supported in workspace names."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The advanced settings in a workspace allow users to manage the contact list, change the license mode, and set the default storage format for Power BI datasets. The contact list is used to notify people about workspace level changes, and the license mode determines the features available in the workspace. The default storage format determines how data is stored in datasets for optimized query performance."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on various features and options in Power BI. It explains the concept of template apps and how they create a special type of workspace. It also discusses the option to store dataflows in Azure Data Lake Storage Gen2. Additionally, it provides instructions on giving users access to workspaces and pinning workspaces for quick access. The document concludes by mentioning the availability of additional resources on workspaces and roles in workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric workspaces are a feature that allows for the division of data lakes into separate containers that can be secured independently. These workspaces extend the capabilities of Power BI workspaces by adding data integration and data exploration features. Roles can be assigned to individuals or user groups, and access to workspaces can be granted by assigning these roles. Users in workspace roles have additional capabilities related to Microsoft Fabric."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The summary states that users can view and read content from KQL databases, query-sets, and real-time dashboards. They can also connect to SQL endpoints of Lakehouse and Data warehouse to read data and shortcuts. Different roles have different capabilities such as reading Lakehouse data, writing or deleting data pipelines and ML models, executing or canceling executions, and viewing execution output. Additional permissions may be required for reading data from shortcut destinations. Admins, members, and contributors can grant viewers granular SQL permissions. The summary also mentions next steps such as creating workspaces, giving users access to workspaces, and information about OneLake and data security."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document explains how to give others access to a workspace in Microsoft Fabric. It states that workspace creators are automatically admins and provides instructions on how to add people or groups to a workspace and assign them different roles. It also mentions that permission changes take effect after the user logs into Microsoft Fabric. The document concludes by suggesting reading about the workspace experience and creating workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The OneLake data hub allows users to easily find, explore, and use data items in their organization. It provides a filterable list of accessible data items, a gallery of recommended items, and various ways to search for items by workspace or domain. The hub also offers an options menu for actions related to each data item. Users can access the data hub by selecting its icon in the navigation pane. The data items list can be filtered by keyword or type, and selecting an item will lead to its details page. The list is organized into three tabs: All (all accessible data items), My data (items owned by the user), and Endorsed (endorsed data items in the organization)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides an overview of the columns in a list of data items in an organization. It includes information such as the name, endorsement status, owner, workspace, last refresh time, next scheduled refresh time, sensitivity, and how to find items by workspace or explore recommended data."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This passage explains how to navigate and interact with data items in an organization's data hub. It mentions recommended data items that have been certified or recently accessed, and how to view details and options for these items. It also discusses the ability to display only data items belonging to a specific domain and how to access the options menu for each item. Additionally, it notes that the Explorer pane may list workspaces that the user doesn't have access to, but only the items they have permission for will be visible."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article explains how to promote and certify items in Microsoft Fabric to increase their visibility and ensure their quality. Promotion allows users to highlight valuable items for others to use, while certification indicates that an item meets the organization's quality standards. Only authorized reviewers can certify items, but anyone with write permissions can promote them. The article provides instructions on how to promote and certify items, as well as how to request certification if you're not authorized."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides instructions on how to promote and certify content in Power BI. It explains how to promote a dataset or item by selecting the \"Promoted\" option in the endorsement section of the settings. It also outlines the process of certifying an item, which involves obtaining write permissions, reviewing the item against certification standards, and selecting the \"Certified\" option in the endorsement section. Additionally, it mentions that users who are not authorized to certify an item can request certification by following specific steps."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides information on how to apply sensitivity labels to Microsoft Fabric items. It explains the importance of sensitivity labels in protecting sensitive content and meeting governance and compliance requirements. The article also includes a note about the pre-release status of Microsoft Fabric and provides a link for applying sensitivity labels in Power BI Desktop."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> To apply sensitivity labels to Fabric items, you need a Power BI Pro or Premium Per User license and edit permissions on the item. If you are unable to apply a sensitivity label or if it is greyed out, you may not have the necessary permissions and should contact your organization's tech support. There are two ways to apply a sensitivity label: through the flyout menu in the item header or in the item settings. The next steps involve reviewing the sensitivity label overview."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = summary['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a unified platform that offers data and analytics solutions for organizations. It combines components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. The platform provides customized user experiences for data engineering, data factory, data science, data visualization, and data governance. It offers extensive analytics capabilities, shared experiences, easy access to assets, a unified data lake, and centralized administration and governance. Microsoft Fabric also combines the benefits of data lakes and data warehouses through the lakehouse architecture. It allows users to provision and configure their own storage accounts and provides a trial capacity for exploring the platform.\n",
      "\n",
      "Microsoft Fabric Home is a centralized hub that allows users to access and manage their items. It provides a search bar, filters, and a user-friendly interface for organizing and working with data. Users can create, edit, delete, and share items directly from the Home page. The top menu bar offers options for orientation, finding items, accessing help, and providing feedback. The Home section organizes items by recent, favorites, and shared with you. Users can navigate to different sections using the navigation bar.\n",
      "\n",
      "The document provides comprehensive information on how to navigate and use Microsoft Fabric, including features of the Data Factory nav pane, Power BI Home canvas, and Fabric Help pane. It covers topics such as searching, sorting, and filtering content, configuring settings, managing storage, creating and administering workspaces, advanced settings, and workspace roles.\n",
      "\n",
      "Instructions are provided on how to view and read the content of KQL databases, connect to SQL endpoints, and read data through these endpoints. The document explains the capabilities of different roles and their ability to read data and execute tasks. It also mentions the need for additional permissions to read data from shortcut destinations and the ability to grant viewers granular SQL permissions.\n",
      "\n",
      "The document also explains how to give others access to a workspace, the different roles in a workspace, and how to enforce row-level security. It provides steps to give access, modify access, and suggests reading about the workspace experience.\n",
      "\n",
      "The OneLake data hub allows users to find, explore, and use Fabric data items. The document explains how to open the data hub, filter data items, find items by workspace, and use the recommended items feature.\n",
      "\n",
      "The document provides an overview of the columns in the Endorsement list and explains how to find data items by workspace and find recommended items. It also discusses the endorsement process, including promotion and certification, and provides instructions on how to request certification.\n",
      "\n",
      "Lastly, the document explains how to apply sensitivity labels to Microsoft Fabric items using sensitivity labels from Microsoft Purview Information Protection. It mentions the prerequisites for applying sensitivity labels and provides instructions on how to apply them.\n",
      "\n",
      "Overall, the document covers various aspects of viewing and reading content, accessing workspaces, using the OneLake data hub, endorsing items, and applying sensitivity labels in Microsoft Fabric.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# While we are using the standard prompt by langchain, you can modify the prompt to suit your needs\n",
    "promptTemplate = \"\"\"You are an AI assistant tasked with summarizing documents. \n",
    "        Your summary should accurately capture the key information in the document while avoiding the omission of any domain-specific words. \n",
    "        Please generate a concise and comprehensive summary that includes details. \n",
    "        Ensure that the summary is easy to understand and provides an accurate representation. \n",
    "        Begin the summary with a brief introduction, followed by the main points. \n",
    "        Please remember to use clear language and maintain the integrity of the original information without missing any important details:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "customPrompt = PromptTemplate(template=promptTemplate, input_variables=[\"text\"])\n",
    "chainType = \"map_reduce\"\n",
    "summaryChain = load_summarize_chain(llm, chain_type=chainType, return_intermediate_steps=True, \n",
    "                                    map_prompt=customPrompt, combine_prompt=customPrompt)\n",
    "summary = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "outputAnswer = summary['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a unified platform that provides data and analytics solutions for organizations. It offers a comprehensive suite of services, including data lake, data engineering, and data integration. Fabric simplifies analytics needs by providing an integrated, end-to-end, and easy-to-use product. It eliminates the need to use multiple services from different vendors. The platform is built on a foundation of Software as a Service (SaaS), ensuring simplicity and integration."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a SaaS foundation that combines components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. It offers various customized user experiences, including Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI. The integration provides benefits such as extensive and deeply integrated analytics, shared experiences across familiar and easy-to-learn interfaces, easy access and reuse of assets for developers, a unified data lake that retains data while using preferred analytics tools, and centralized administration and governance. With Microsoft Fabric, data and services are seamlessly integrated, and core enterprise capabilities can be centrally configured. Permissions and data sensitivity labels are automatically applied across all services."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> and model deployment capabilities. For more information, see What is Data Science in Microsoft Fabric?\n",
       "\n",
       "Data Visualization - Data Visualization experience provides a rich set of tools and\n",
       "\n",
       "capabilities to create interactive and insightful visualizations. It includes\n",
       "\n",
       "Power BI, which allows you to create interactive reports and dashboards, and\n",
       "\n",
       "Azure Synapse Studio, which provides a collaborative environment for data\n",
       "\n",
       "exploration and visualization. For more information, see What is Data Visualization\n",
       "\n",
       "in Microsoft Fabric?\n",
       "\n",
       "Data Governance - Data Governance experience helps you manage and govern your\n",
       "\n",
       "data assets effectively. It includes capabilities such as data cataloging, data\n",
       "\n",
       "classification, and data lineage. It also integrates with Azure Purview to provide\n",
       "\n",
       "end-to-end data governance. For more information, see What is Data Governance in\n",
       "\n",
       "Microsoft Fabric?\n",
       "\n",
       "Summary:\n",
       "\n",
       "Microsoft Fabric is a comprehensive set of analytics experiences that allows creators to focus on their work without worrying about the underlying infrastructure. It includes data engineering, data factory, data science, data visualization, and data governance components. Each component is tailored to specific tasks and personas. The data engineering experience provides a Spark platform for large-scale data transformation, while the data factory combines Power Query with Azure Data Factory for data integration. The data science experience enables the building and deployment of machine learning models. The data visualization experience offers tools for creating interactive visualizations, and the data governance experience helps manage and govern data assets effectively."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides an overview of the key features and capabilities of Microsoft Fabric, including data science, data warehousing, real-time analytics, and Power BI. \n",
       "\n",
       "Data science in Microsoft Fabric allows data scientists to enrich organizational data with predictions and enables business analysts to integrate those predictions into their BI reports, shifting from descriptive to predictive insights. \n",
       "\n",
       "The data warehousing experience in Microsoft Fabric offers industry-leading SQL performance and scale. It separates compute from storage, allowing independent scaling of both components. It also stores data in the open Delta Lake format. \n",
       "\n",
       "Real-Time Analytics in Microsoft Fabric is designed for analyzing observational data collected from various sources such as apps, IoT devices, and human interactions. This data is often semi-structured and comes in high volume with shifting schemas, making it challenging for traditional data warehousing platforms to handle. \n",
       "\n",
       "Power BI is a leading Business Intelligence platform that allows business owners to access all the data in Microsoft Fabric quickly and intuitively, enabling them to make better decisions based on data. \n",
       "\n",
       "Overall, Microsoft Fabric provides a comprehensive solution for data science, data warehousing, real-time analytics, and business intelligence, empowering organizations to leverage their data effectively."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> and teams often create. It provides a centralized and organized data repository that can be accessed by all authorized users within the organization.\n",
       "\n",
       "Lakehouse\n",
       "\n",
       "The lakehouse architecture is a combination of data lake and data warehouse concepts. It allows organizations to have the best of both worlds by providing the scalability and flexibility of a data lake, along with the structured querying and performance optimizations of a data warehouse.\n",
       "\n",
       "Microsoft Fabric brings the lakehouse architecture to the Fabric platform, enabling organizations to leverage the benefits of both OneLake and data warehousing. This allows for efficient data processing, analytics, and reporting, while also ensuring data integrity and consistency.\n",
       "\n",
       "The unification of OneLake and lakehouse architecture in Microsoft Fabric provides a comprehensive solution for big data analytics. It simplifies the data management process, eliminates data silos, and enables organizations to derive valuable insights from their data.\n",
       "\n",
       "In conclusion, Microsoft Fabric is a powerful platform that combines the OneLake and lakehouse architecture to provide a unified and comprehensive solution for big data analytics. It simplifies data management, eliminates data silos, and enables organizations to leverage the benefits of both data lakes and data warehouses."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> OneLake is a unified storage system that allows developers to provision and configure their own isolated storage accounts. It simplifies data sharing and ensures compliance with policy and security settings. OneLake is hierarchical in nature and is built into Microsoft Fabric, eliminating the need for upfront provisioning. It provides a single-pane-of-glass file-system namespace that spans across users, regions, and clouds. The data in OneLake is divided into manageable containers. The tenant, which is at the top level of the hierarchy, can create multiple workspaces, which can be thought of as folders. Each workspace can have multiple lakehouses, which are collections of files, folders, and tables representing a database over a data lake. Every developer and business unit in the tenant can create their own workspaces and lakehouses instantly."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> OneLake is a feature in Microsoft Fabric that allows users to ingest, process, analyze, and collaborate on data. It is similar to OneDrive in Office and is prewired to all Microsoft Fabric compute experiences. OneLake serves as the native store for various experiences such as Data Engineering, Data Warehouse, Data Factory, Power BI, and Real-Time Analytics. Users can easily mount existing PaaS storage accounts into OneLake using the Shortcut feature, without the need to migrate or move any data. Shortcuts also enable easy sharing of data between users and applications without duplication. Additionally, shortcuts extend to other storage systems, allowing users to compose and analyze data across clouds with intelligent caching that reduces egress costs. To get started with Microsoft Fabric, users can create a workspace and access their items from the Microsoft Fabric Home page. End-to-end tutorials are available to help users navigate and utilize Microsoft Fabric effectively. The Microsoft Fabric (Preview) trial provides access to the Fabric product experiences."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to start a Fabric (Preview) trial. The trial lasts until Fabric General Availability (GA), unless canceled, and after GA, it converts to the GA version and is extended for 60 days. Existing Power BI users can skip to starting the trial, while new users need to sign up for a Power BI free license before starting the Fabric (Preview) trial. To start the trial, users need to open the Fabric homepage, select the Account manager, choose Start trial, agree to the terms if prompted, and wait for the trial capacity to be ready. Once ready, users will receive a confirmation message and can begin working in Fabric."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document explains that as a user, you have a Fabric (Preview) trial that includes a Power BI individual trial and a Fabric (Preview) trial capacity. It mentions that if your Power BI administrator enables the preview of Microsoft Fabric for the tenant, you can start a Fabric (Preview) trial by trying to create a Fabric item in a workspace that doesn't support Fabric items. This will upgrade your workspace to a trial capacity workspace. \n",
       "\n",
       "The document also defines a trial capacity as a distinct pool of resources allocated to Microsoft Fabric, with the size of the capacity determining the amount of computation power reserved for users. It states that with a Fabric (Preview) trial, you have full access to all Fabric experiences and features, as well as OneLake storage up to 1 TB. You can create workspaces for projects that support Fabric capabilities and collaborate with others in the same Fabric trial capacity."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Fabric is a platform that allows users to share and collaborate on datasets, warehouses, and notebooks. Users can also create analytics solutions using these items. To start using Fabric, users need to add items to their workspace or create a new workspace. They can then assign that workspace to their trial capacity using the \"Trial\" license mode. The trial capacity provides users with 64 capacity units (CU), which allows them to consume 64x60 CU seconds every minute. The consumption rate of capacity units varies depending on the function being used. If the consumption exceeds the capacity size, Microsoft will slow down the experience. There is no limit on the number of workspaces or items that can be created, but the availability of capacity units and the consumption rate are the constraints."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> In order to administer your trial capacity in Fabric, you have access to a detailed report through the Capacity Metrics app. If you decide to end your Fabric trial, all workspaces and their contents will be deleted, and you will no longer be able to create workspaces or share Fabric items with other users. Additionally, canceling your trial may prevent you from starting another trial in the future. Power BI administrators have the ability to enable or disable trials for paid features in both Power BI and Fabric. However, it's important to consider the impact of changing this setting, as it applies to all users or specific security groups within the tenant. Each trial user is the capacity admin for their own trial capacity, and Microsoft currently does not support multiple capacity administrators per trial capacity."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document discusses considerations and limitations related to starting a trial in Power BI. It mentions that if the \"Start trial\" button is not visible in the Account manager, it may be disabled by the Power BI administrator. In such cases, users should contact their administrator to request access or start a trial using their own tenant. Existing Power BI trial users can start a Fabric (Preview) trial by attempting to create a Fabric item. If the Fabric (Preview) feature is disabled, users should contact their administrator. If the \"Start trial\" button is visible, users may not be able to start a trial if their tenant has exhausted its trial capacity limit. In such cases, users have the options to purchase a Fabric capacity from Azure, request another trial capacity user to share their capacity, or contact their Power BI administrator to increase the tenant trial capacity limits. Additionally, the document mentions that in Workplace settings, it is not possible to assign a workspace to the trial capacity."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> up for the trial, you can access the Fabric landing page and use the features available in the trial capacity. However, the Fabric (Preview) trial is different because it allows you to create and manage Fabric items in addition to accessing the Fabric landing page. This means that you can create and manage workspaces, datasets, reports, and dashboards within the Fabric (Preview) trial."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Fabric (Preview) trial allows users to store Fabric workspaces and items and run Fabric experiences. The trial capacity is free and does not require customization. However, private links and private access are not supported during the trial. Autoscale is also not available in the trial capacity. Existing Synapse users should note that the trial is different from a Proof of Concept (POC) and does not require financial investment or customization. For existing Power BI users, the rules and licenses remain the same, but a Fabric capacity is required to access non-Power BI experiences and items. It is important to mention that an Azure subscription is not necessary to start the Fabric (Preview) trial, but a paid Fabric capacity can be purchased if the user has an existing Azure subscription."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document provides information on how to migrate existing workspaces into a trial capacity using workspace settings. By selecting \"Trial\" as the license mode, users can easily migrate their workspaces. The document also introduces Microsoft Fabric terminology and provides definitions for terms used in Synapse Data Warehouse, Synapse Data Engineering, Synapse Data Science, Synapse Real-Time Analytics, Data Factory, and Power BI. It explains that capacity refers to a dedicated set of resources available for use, and different items consume different capacity. The document emphasizes that Microsoft Fabric is currently in preview and may undergo substantial modifications before its release."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> debugging code, and visualizing data. Notebooks are used for data exploration,\n",
       "\n",
       "data preparation, and data transformation tasks. Notebooks can be created and\n",
       "\n",
       "executed in a workspace. For more information, see Notebooks article.\n",
       "\n",
       "Spark job definition: A Spark job definition is a script or code that defines a set of\n",
       "\n",
       "instructions for the Apache Spark engine to execute. It can be written in different\n",
       "\n",
       "languages such as Scala, Python, or SQL. Spark job definitions are used for\n",
       "\n",
       "performing data processing, data analysis, and data transformation tasks on large\n",
       "\n",
       "datasets. They can be created and submitted in a workspace. For more\n",
       "\n",
       "information, see Spark job definitions article.\n",
       "\n",
       "In summary, Synapse provides different items such as the Data Engineering\n",
       "\n",
       "experience, which includes the lakehouse, notebook, and Spark job definition\n",
       "\n",
       "items. A tenant is a single instance of Fabric aligned with an Azure Active\n",
       "\n",
       "Directory. A workspace is a collection of items that brings together different\n",
       "\n",
       "functionality in a single environment designed for collaboration. The lakehouse is\n",
       "\n",
       "a collection of files, folders, and tables used for big data processing. Notebooks\n",
       "\n",
       "are interactive programming tools used for data exploration and transformation.\n",
       "\n",
       "Spark job definitions are scripts or code used for data processing and analysis on\n",
       "\n",
       "large datasets. These items can be created and executed within a workspace."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> to various data sources and destinations. These connectors enable you to easily ingest and transform data from different sources, such as Azure Blob Storage, Azure Data Lake Storage, Azure SQL Database, and more.\n",
       "\n",
       "Data Flow: Data Flow is a cloud-based data integration service that allows you to visually design and execute data transformations at scale. It provides a code-free experience for building data transformation logic using a drag-and-drop interface, making it easier for data engineers and data scientists to process and manipulate data.\n",
       "\n",
       "Data Factory Monitoring: Data Factory provides monitoring capabilities that allow you to track the execution of your data pipelines and gain insights into their performance. You can monitor the status of your pipelines, view detailed logs, and set up alerts to be notified of any issues or failures.\n",
       "\n",
       "Data Factory Visual Tools: Data Factory offers visual tools that enable you to design and manage your data pipelines using a graphical interface. These tools provide a user-friendly experience for creating and orchestrating complex data workflows, making it easier to collaborate with your team and ensure the smooth execution of your data integration processes.\n",
       "\n",
       "In summary, Data Factory is a powerful tool for data engineers and data scientists to explore, process, and transform data. It offers a wide range of connectors to easily connect to various data sources and destinations, and provides a code-free experience for building data transformation logic. With its monitoring and visual tools, Data Factory allows for efficient collaboration and management of data pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides an overview of various data-related features and tools in Microsoft Fabric. It mentions connectors that allow users to connect to different types of data stores and transform the data. Data pipelines are used for orchestrating data movement and transformation. Dataflow Gen2, also known as Dataflows in Fabric, provides a low-code interface for ingesting and transforming data from various sources. It offers extra capabilities compared to Dataflow Gen1 in Power BI or Azure Data Factory. Data Wrangler is a notebook-based tool that allows users to conduct exploratory data analysis and perform data-cleansing operations. Lastly, machine learning experiments are the primary unit of organization and control for related machine learning runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information about various terms related to machine learning models and data warehousing in Synapse. \n",
       "\n",
       "A machine learning model is a file trained to recognize patterns, using an algorithm to reason and learn from a data set. \n",
       "\n",
       "In MLflow, a run refers to a single execution of model code, and tracking is based on experiments and runs. \n",
       "\n",
       "Synapse Data Warehouse is a traditional data warehouse that supports full transactional T-SQL capabilities. \n",
       "\n",
       "Synapse Real-Time Analytics includes a KQL database, which holds data for executing KQL queries. \n",
       "\n",
       "The KQL Queryset is used to run queries, view and manipulate query results, and save or share queries with others."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> version of Microsoft Fabric and may be subject to change.\n",
       "\n",
       "The Microsoft Fabric event streams feature is a centralized place in the Fabric platform that captures, transforms, and routes real-time events to destinations. It includes streaming data sources, ingestion destinations, and an event processor for transformation. OneLake shortcuts are embedded references in OneLake that connect to existing data without copying it. \n",
       "\n",
       "The article provides a list of end-to-end tutorials available in Microsoft Fabric. These tutorials cover the entire process from data acquisition to data consumption and aim to help users understand the Fabric UI, different experiences supported by Fabric, and integration points. It is important to note that Microsoft Fabric is currently in preview and subject to change."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides a list of tutorials that cover various experiences within the Fabric platform. The tutorials include Lakehouse, Data Science, Real-Time Analytics, Data Warehouse, and Power BI. In the Lakehouse tutorial, you will learn how to ingest, transform, and load data into a lakehouse and analyze sales data. The Data Science tutorial focuses on exploring, cleaning, and transforming a taxicab trip dataset and building a machine learning model. The Real-Time Analytics tutorial teaches you how to use streaming and query capabilities to analyze the New York Yellow Taxi trip dataset. The Data Warehouse tutorial guides you in building an end-to-end data warehouse for a fictional company. Lastly, the Power BI tutorial helps you create a dataflow, pipeline, dimensional model, and generate reports."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on using data pipelines and dataflows to ingest and transform data, as well as automation and notification features for data integration. It also discusses the capabilities of the Data Science experience and provides examples of how ML models can address common business problems. Additionally, there is a tutorial on building a machine learning model to analyze and predict avocado prices in the US. The document also includes a decision guide for choosing between copy activity, dataflow, or Spark for workloads using Microsoft Fabric, with example scenarios provided. It is important to note that Microsoft Fabric is currently in preview and the information provided may be subject to changes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> and gold layers for data ingestion, transformation, and storage. He also wants to use a low-code or no-code development interface for ease of use. Leo should consider using Dataflow Gen 2, which supports over 30 connectors for data ingestion and over 18 connectors for data storage. It also provides a wizard or canvas interface for development, making it easy for Leo to ingest and transform data without writing extensive code.\n",
       "\n",
       "Scenario 2\n",
       "\n",
       "Sarah, a data scientist, needs to process and analyze large volumes of data using Spark. She wants to leverage the power of Spark libraries and native Spark functions for complex transformations. Sarah should use Dataflow Gen 2, as it supports hundreds of Spark libraries and provides over 300 transformation functions. This will allow her to efficiently process and analyze data using the full capabilities of Spark.\n",
       "\n",
       "Scenario 3\n",
       "\n",
       "Mark, a data developer, wants to build scalable data pipelines using Spark. He needs to integrate data from various sources and perform transformations before loading it into a data lake or Azure SQL database. Mark should use Dataflow Gen 2, as it supports over 150 connectors for data ingestion and provides support for native Spark and open-source libraries. This will allow him to easily integrate data from different sources and perform complex transformations before loading it into the desired destination.\n",
       "\n",
       "In summary, Dataflow Gen 2 is a powerful tool for data ingestion, transformation, and processing. It supports a wide range of connectors, provides a low-code or no-code development interface, and offers extensive support for Spark libraries and functions. Whether you're a data engineer, data scientist, or data developer, Dataflow Gen 2 can help you efficiently work with your data and build scalable data pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Leo wants to process data on a schedule and prefers a drag and drop UI with minimal coding. He needs to get raw data from various sources into a consolidated lakehouse. He selects pipeline copy activity to copy the data, both historical and incremental, to the lakehouse. This activity allows for high scale data ingestion and can move petabyte-scale data. Mary, on the other hand, is responsible for cleaning and applying business logic to data before loading it into multiple destinations for reporting teams."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> In summary, Mary, an experienced Power Query user, decides to use Dataflow Gen 2 as her preferred transformation option. Dataflows allow for easy data ingestion from various sources and offer over 300 data transformation options. Mary finds the highly visual user interface of Dataflow Gen 2 appealing.\n",
       "\n",
       "On the other hand, Adam, a data engineer at a retail company, chooses Spark as the best option for building data pipelines. Spark is a distributed computing platform that can process large amounts of data in parallel. Adam writes a Spark application in Python or Scala to extract, transform, and load customer review data from OneLake. The application cleanses and transforms the data before writing it to Delta tables in the lakehouse. This data is then available for downstream analytics.\n",
       "\n",
       "The next steps for both Mary and Adam would involve using the transformed data for their respective analytics purposes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides a decision guide for choosing between a data warehouse and a lakehouse using Microsoft Fabric. The properties of a data warehouse include unlimited data volume and structured data, while a lakehouse supports unstructured, semi-structured, and structured data. The primary developer persona for a data warehouse is a data warehouse developer or SQL engineer, while a lakehouse is suitable for data engineers and data scientists. The primary development skills for a data warehouse are SQL, while a lakehouse supports Spark (Scala, PySpark, Spark SQL, R) and SQL. Data in a data warehouse is organized by databases, schemas, and tables, while a lakehouse organizes data by folders, files, databases, and tables. Both data warehouses and lakehouses support read and write operations, with data warehouses using T-SQL and lakehouses using Spark, T-SQL, and Power BI. Data warehouses also support multi-table transactions. The primary development interface for a data warehouse is Power BI Datamart, while a lakehouse does not have one. Data warehouse provides security features, while a lakehouse does not."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> In this document, we are comparing the features and capabilities of a lakehouse and a data warehouse in Microsoft Fabric. The comparison includes SQL scripts, Spark notebooks, Power BI, Spark job definitions, object level (table, view, function, stored procedure, etc.), column level, row level, DDL/DML, built-in RLS editor, access data via shortcuts, query across items and warehouse tables, and scenarios.\n",
       "\n",
       "The document provides a scenario where Susan, a professional developer, needs to decide whether to build a data warehouse or a lakehouse in Microsoft Fabric. Susan is familiar with SQL and has experience building data warehouses on relational database engines. The primary consumers of the data are also skilled with SQL and SQL analytical tools. Based on these factors, Susan decides to use a data warehouse, which allows the team to interact primarily with T-SQL and also allows Spark users in the organization to access the data."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> In Scenario 2, Rob, a data engineer, needs to store and model a large amount of data in Fabric. The team has a mix of PySpark and T-SQL skills, with most team members using T-SQL for querying. Rob decides to use a lakehouse, which allows the team to leverage their diverse skills while allowing T-SQL experts to consume the data.\n",
       "\n",
       "In Scenario 3, Ash, a Power BI developer, needs to build a data product for a business unit. They are familiar with Excel, Power BI, and Office, but don't have the skills for a data warehouse or lakehouse. They review their skills and data volume requirements and decide that a self-service, no-code solution with data volume under 100 GB is suitable. Ash works with business analysts who are familiar with Power BI and Microsoft Office, and they already have a Premium capacity subscription. The primary consumers of the data are expected to be analysts familiar with no-code tools."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> will change as you create and access more items.\n",
       "\n",
       "Navigating to your items\n",
       "\n",
       "To navigate to your items, follow these steps:\n",
       "\n",
       "1. Sign in to Microsoft Fabric Home.\n",
       "2. On the Home page, you'll see a list of items that you have access to.\n",
       "3. Use the search bar at the top to search for specific items.\n",
       "4. Use the filters on the left side to narrow down your search.\n",
       "5. Click on an item to open it and view its details.\n",
       "6. Use the navigation bar at the top to switch between different sections, such as\n",
       "\n",
       "   Workspaces, Datasets, Pipelines, and Triggers.\n",
       "\n",
       "Actions you can take\n",
       "\n",
       "On the Home page, you can perform various actions on your items, including:\n",
       "\n",
       "- Creating new items: You can create new workspaces, datasets, pipelines, and\n",
       "\n",
       "  triggers directly from the Home page.\n",
       "\n",
       "- Editing items: You can edit the details and settings of your items.\n",
       "\n",
       "- Deleting items: You can delete items that you no longer need.\n",
       "\n",
       "- Sharing items: You can share your items with other users or groups.\n",
       "\n",
       "- Viewing item history: You can view the history of changes made to an item.\n",
       "\n",
       "- Accessing documentation: You can access documentation and resources related to\n",
       "\n",
       "  the item you're viewing.\n",
       "\n",
       "Conclusion\n",
       "\n",
       "Microsoft Fabric Home provides a centralized location for accessing and managing your\n",
       "\n",
       "items in Microsoft Fabric. It allows you to easily navigate to your items, perform various\n",
       "\n",
       "actions on them, and access relevant documentation. By following the steps outlined in\n",
       "\n",
       "this article, you can effectively use Microsoft Fabric Home to organize and work with\n",
       "\n",
       "your data."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document discusses the use of Microsoft Fabric and how to navigate and access items within it. It explains that Home is a central hub where users can access their items, but it is not workspace-specific. The document also mentions that the term \"item\" in Microsoft Fabric refers to various things such as apps, lakehouses, warehouses, and reports. It suggests that users can navigate directly to a workspace using the nav pane and workspace selector. To open Home, users can select it from the top of the left navigation pane. The document emphasizes that the most important content is easily accessible on Home and suggests using global search if the Home canvas gets crowded. It also mentions that Power BI Home is different from other product experiences. The document further highlights the left navigation pane as a tool for accessing different views of items and creator resources, as well as the selector for switching product experiences."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document discusses the top menu bar in Microsoft Fabric, which includes options for orienting yourself, finding items, accessing help, and providing feedback to Microsoft. It also mentions the Account manager control, which is important for managing your Fabric trial and accessing account information. The document then explains the options for creating new items and provides links to recommended content to help users get started. The Home section of Microsoft Fabric organizes your items by recent, favorites, and items shared with you by colleagues. It notes that only the content you have access to will appear on your Home, and if your subscription or license changes to one with less access, you will be prompted to start a trial or upgrade your license. The document also mentions different ways to locate and view your content, including searching and using the navigation pane on the left side of the interface."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document explains how to use the Data Factory nav pane and find and open workspaces. The nav pane is a tool that organizes actions and helps users quickly access their items. The bottom section of the nav pane displays workspaces, and users can select one to open. By default, the nav pane includes the Workspaces selector and \"My workspace,\" but the name changes when a workspace is opened. The nav pane remains visible as users navigate through different areas of Microsoft Fabric. Workspaces are collaborative spaces where users can create collections of items. Users can search for workspaces by name or owner, or they can select the Workspaces icon in the nav pane to choose a workspace to open. The workspace opens on the canvas, and its name is listed."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document explains how to navigate and use Microsoft Fabric, a platform that allows users to access and manage their workspaces. The main points include:\n",
       "\n",
       "1. Workspaces: When you open a workspace, you can see its content, which includes notebooks, pipelines, reports, and lakehouses.\n",
       "\n",
       "2. Experience Selector: In the bottom left corner, there is an experience selector icon. Clicking on it allows you to choose and open different Microsoft Fabric product experiences.\n",
       "\n",
       "3. Search, Sort, and Filter: Microsoft Fabric provides various ways to search, sort, and filter content. You can search by item, name, keyword, workspace, and more.\n",
       "\n",
       "4. Context Sensitive Help Pane: The Help icon (?) opens the contextual Help pane, where you can search for answers to your questions. The Help pane is updated based on the selected view in the nav pane. It provides articles and information about the features of the current screen.\n",
       "\n",
       "5. Community Posts: If there are community posts related to the current view, they are displayed under Forum topics in the Help pane.\n",
       "\n",
       "6. Using Help Pane: You can leave the Help pane open while you work and use the suggested topics to learn how to use Microsoft Fabric features and terminology. Alternatively, you can close the Help pane to save screen space.\n",
       "\n",
       "Overall, the document provides guidance on navigating and utilizing Microsoft Fabric effectively, including accessing workspaces, selecting experiences, searching for content, and using the context-sensitive Help pane."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Home canvas is where you can find what you need. It displays your recent files, recommended files, and files shared with you. You can also create new files and folders from the canvas. The canvas is customizable, allowing you to arrange and organize your files and folders in a way that suits your needs. Additionally, you can use the search bar at the top of the canvas to quickly find specific files or folders."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document discusses two topics: Power BI Home and the Fabric Help pane. \n",
       "\n",
       "Power BI Home is a canvas that updates as you select different items. It displays options for creating new items, recommended items, recents, favorites, and shared content. If you choose the Show less view, the New section is collapsed. When you create a new item, it is saved in your My workspace unless you've selected a different workspace from Workspaces. Power BI Home is different from other product experiences and you can learn more about it by visiting Power BI Home.\n",
       "\n",
       "The Fabric Help pane is a feature-aware tool that provides articles about actions and features available on the current Fabric screen. It also functions as a search engine for finding answers in the Fabric documentation and community forums. However, it's important to note that Microsoft Fabric is currently in PREVIEW, meaning it is a prerelease product that may undergo significant modifications before its official release. Microsoft does not provide any warranties, expressed or implied, regarding the information provided about Fabric."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Help pane in Fabric is the default view when opened without any search terms. It provides recommended topics, relevant resources, and links for other resources. The Help pane has three sections: feature-aware documents, forum topics, and other resources. The feature-aware documents section groups documents by the features available on the current screen. The forum topics section shows topics from the community forums related to the features on the current screen. The other resources section provides links for feedback and support. The Help pane also functions as a search engine, allowing users to enter keywords to find relevant information and resources from Microsoft documentation and community forum topics. It is a useful tool for learning and getting started with Fabric, as the feature-aware documents update based on user selections and location in Fabric. Users can explore Fabric and read the feature-aware documents to learn how to use it effectively."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> version of the software and may be subject to change.\n",
       "\n",
       "The Help pane in Microsoft Fabric has been updated with articles to help users learn about the data hub. To access the Help pane, click on the ? icon in the upper-right corner of Fabric. From there, you can browse through recent features, open documents in separate browser tabs, and explore forum posts for additional context. You can also search the Microsoft documentation and community forums by entering a keyword in the search pane. To return to the default display of the Help pane, simply select the arrow, and to close the Help pane, click on the X icon in the upper-right corner. If you still need assistance, you can ask the community or submit a question, and if you have an idea for a new feature, you can submit it as well. Additionally, there is a global search feature available to help you find specific content within Microsoft Fabric. As you create and share more items in Fabric, searching, filtering, and sorting can become useful tools to manage long lists of content. It's important to note that Microsoft Fabric is currently in preview, so the information provided may be subject to change."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document explains how to use the search function in Microsoft Fabric to find items by title, name, or keyword. It states that searching can be faster than scrolling or sorting through content. The search function is available on the Home page and other areas of Microsoft Fabric. Users can search for items, creators, keywords, workspaces, or content shared by colleagues. Additionally, there is a \"Filter by keyword\" feature that allows users to narrow down the content on their canvas using keywords. This filtering applies to the current view only."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to sort and filter content lists in Microsoft Fabric. Sorting is useful for finding items in long lists, and it can be done by selecting the arrow next to the \"Name\" column. Sorting criteria can also be set for workspaces by selecting a column header. Not all columns can be sorted, so it's important to hover over the column headings to check. Filtering is another way to quickly locate content, and it can be done by selecting the \"Filter\" option in the upper right corner. The available filters depend on the location in Microsoft Fabric. The document also includes next steps for finding Fabric items from the Home page and starting a Fabric trial. It's important to note that Microsoft Fabric is currently in preview and may undergo substantial modifications before its release."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The Fabric settings pane is a feature that provides access to various settings that can be configured. To open the Fabric settings pane, users can select the gear icon in the Fabric portal header. The preferences section allows individual users to set their user preferences, specify the language of the Fabric user interface, manage their account and notifications, and configure settings for their personal use throughout the system. The settings pane also includes links to different settings pages. The general settings page allows users to set the display language for the Fabric interface and parts of visuals. The notifications settings page allows users to view their subscriptions and alerts. The item settings page allows users to configure settings specific to each item type. The developer settings page allows users to configure developer mode settings. Additionally, the resources and extensions section provides links to pages where users can manage personal/group storage and access Power BI settings."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> as dashboards, datasets, workbooks, reports, datamarts, and dataflows. In the current workspace, you can manage connections, on-premises data gateways, embed codes, and virtual networks data gateways. You can also migrate your Azure Analysis Services datasets to Power BI Premium. The governance and insights section provides links to help admins and users with their admin, governance, and compliance tasks. Additionally, there is a link to the Fabric admin portal where admins can perform management tasks and configure Fabric tenant settings. Power BI admins can also access the Microsoft Purview hub, which provides insights about sensitive data and links to Purview governance and compliance capabilities. Workspaces in Power BI are essential for collaboration and managing various Power BI items."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This article provides information on how to work with workspaces, including tips on pinning favorite workspaces, using different workspace roles for permissions management, and navigating to the current workspace. It also explains how to manage workspace configurations and update workspace settings as an admin. Additionally, it covers the contact list feature for receiving notifications about workspace activity. The article emphasizes the importance of the current workspace and highlights the various settings that workspace admins can manage, such as general settings, contact lists, OneDrive integration, licenses, Azure connections, storage, and other specific settings for different experiences."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> library is being used in the workspace. This ensures that the same set of users have access to both the workspace and the files in the library. However, it is important to note that Microsoft Fabric does not automatically synchronize permissions between the two. Therefore, if you want to give access to the workspace to the same Microsoft 365 Group, you will need to manually add the group to the workspace settings."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document provides instructions on how to configure storage and manage workspace access in Microsoft 365. It explains that you can configure OneDrive in workspace settings by entering the name of the Microsoft 365 group. Workspaces are created in your organization's shared capacity by default, but can be assigned to other capacities if available. The document also mentions that workspace admins can configure dataflow storage to use Azure Data Lake Gen 2 storage and Azure Log Analytics connection for collecting usage and performance logs. Additionally, it discusses the option to bring your own storage to dataflows with Azure Data Lake Gen 2 integration. Finally, it mentions that system storage is used for managing dataset storage in individual or shared workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to manage and administer workspaces in Microsoft Fabric. It explains that in the system storage, users can view their storage usage and delete items to free up space. However, it warns that deleting a dataset may affect reports and dashboards that are based on it. The document also explains how to delete a workspace, which removes all its contents for all group members and removes the associated app from AppSource. Additionally, it mentions that Microsoft Fabric admins have the ability to manage, recover, and delete workspaces, and they can audit activities such as creating and deleting folders."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to create workspaces in Microsoft Fabric. Workspaces are collections of items such as lakehouses, warehouses, and reports. To create a workspace, you need to select \"Workspaces\" and then \"New workspace\". You can give the workspace a unique name, provide a description (optional), and assign it to a domain (optional). After completing these steps, you can continue to the advanced settings or select \"Apply\". It is important to note that there are limitations to be aware of, such as the maximum number of datasets and reports per dataset in a workspace, as well as certain special characters not being supported in workspace names when using an XMLA endpoint. Additionally, a user or service principal can be a member of up to 1,000 workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The document discusses the advanced settings in Power BI. It mentions that the contact list is a place where you can add names of people who will receive system email notifications for workspace level changes. The first workspace admin is the default contact, but you can add other users or groups as needed. The document also mentions that different license modes provide different sets of features for the workspace, and you can change the license type in the workspace settings. However, there may be some migration effort required. It also notes that if you want to downgrade the workspace license type from Premium capacity to Pro, you need to remove any non-Power BI Fabric items in the workspace. Finally, it mentions that Power BI datasets can store data in a highly compressed in-memory cache for optimized query performance, and with Premium capacities, large datasets can be enabled with the Large dataset storage format setting. The dataset size is then limited by the Premium capacity size or the default limit."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Power BI template apps are designed for sharing outside of your organization and create a special type of workspace. Once created, this workspace cannot be reverted back to a normal workspace. Data used with Power BI is stored in internal storage provided by Power BI by default, but with the integration of dataflows and Azure Data Lake Storage Gen 2 (ADLS Gen2), you can store your dataflows in your organization's ADLS Gen2 account. To collaborate with others, you can give them access to your workspace and assign them roles. You can also pin your favorite workspaces to the top of the workspace flyout list for quick access."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric workspaces are a feature of OneLake that allows the data lake to be divided into separate containers for independent security. These workspaces extend the roles in Power BI by adding new capabilities such as data integration and exploration. Roles can be assigned to individuals or user groups, and there are four workspace roles: Admin, Member, Contributor, and Viewer. When assigning roles to user groups, the highest level of permission is granted if a user is in multiple groups. Users in workspace roles have additional capabilities related to Microsoft Fabric, in addition to the existing Power BI capabilities."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to view and read the content of KQL databases, KQL query-sets, and real-time dashboards. It also explains how to connect to SQL endpoints of Lakehouse and Data warehouse and read data and shortcuts through these endpoints. The document outlines the capabilities of different roles, such as Admin, Member, Contributor, and Viewer, including their ability to read Lakehouse data through Lakehouse explorer and execute or cancel execution of various tasks like notebooks, Spark job definitions, ML models and experiments, and data pipelines. It also mentions the need for additional permissions to read data from shortcut destinations and the ability of Admins, Members, and Contributors to grant viewers granular SQL permissions. The document concludes with information on next steps, including creating workspaces, giving users access to workspaces, and details on OneLake security, shortcuts, data warehouse security, data engineering security, and data science roles and permissions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides instructions on how to give others access to a workspace in Microsoft Fabric. The different roles in a workspace are explained, and it is noted that workspace creators are automatically admins. To enforce row-level security (RLS) on Power BI items for Microsoft Fabric Pro users, they should be assigned the Viewer Role. It is important to note that any changes made to workspace access for a user or group will only take effect after the user logs into Microsoft Fabric again. The steps to give access to a workspace are as follows: \n",
       "1. As an admin or member in the workspace, go to the workspace page and click on \"Manage Access\" on the command bar or the \"More options\" menu.\n",
       "2. Select \"Add people or groups\".\n",
       "3. Enter the name or email of the person or group, choose a role (admin, member, contributor, or viewer), and click \"Add\". It is possible to add security groups, distribution lists, Microsoft 365 groups, or individuals to the workspace.\n",
       "4. Access can be viewed and modified later if needed. Use the search box to find people or groups with existing access, and select the drop-down arrow to modify their role.\n",
       "The document also suggests reading about the workspace experience and provides a link to create workspaces."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> the OneLake data hub, please refer to the article.\n",
       "\n",
       "The OneLake data hub is a tool that allows users to easily find, explore, and use the Fabric data items in their organization. It provides a filterable list of all the data items that the user has access to, as well as a gallery of recommended data items. Users can also find data items by workspace and display only the data items of a selected domain. The data hub also offers an options menu with various actions that can be performed on the data item.\n",
       "\n",
       "To open the data hub, users can select the OneLake data hub icon in the navigation pane. The data items list displays all the data items that the user has access to. Users can filter the list by keyword or data-item type using the filters at the top of the list. Selecting the name of an item will take the user to the item's details page, while hovering over an item will reveal an options menu.\n",
       "\n",
       "The data items list has three tabs: \"All,\" \"My data,\" and \"Endorsed in your organization.\" The \"All\" tab displays all data items that the user is allowed to find. The \"My data\" tab shows data items that the user owns. The \"Endorsed in your organization\" tab lists endorsed data items in the user's organization, with certified data items listed first, followed by promoted data items.\n",
       "\n",
       "In conclusion, the OneLake data hub is a comprehensive tool for discovering and working with data items in an organization. It offers various features such as filtering, recommended items, workspace-based search, and domain-specific display. Users can easily access and interact with data items through the options menu."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides an overview of the columns in the Endorsement list. The columns include Name, Endorsement, Owner, Workspace, Refreshed, Next refresh, and Sensitivity. The Name column displays the data item name and can be clicked to view more details. The Endorsement column shows the endorsement status of the data item. The Owner column displays the data item owner, which is only listed in the All and Endorsed tabs. The Workspace column indicates the workspace where the data item is located. The Refreshed column shows the last refresh time of the data item, rounded to the hour, day, month, and year. The Next refresh column displays the time of the next scheduled refresh, but this is only available in the My data tab. The Sensitivity column indicates the sensitivity of the data item, if set, and can be clicked to view the sensitivity label description. Additionally, the document explains how to find data items by workspace and how to find recommended items using the tiles in the data hub. It is important to note that the Explorer pane may list workspaces that you don't have access to, but if you select such a workspace, only the items you have access to will be displayed in the data items list."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> access to will be displayed in the data hub.\n",
       "\n",
       "The data hub is a tool that displays data items in tiles. These tiles provide information about the items and allow you to perform various actions on them. You can filter the displayed items by selecting a specific domain. Each item also has an options menu that allows you to manage settings and permissions. It's important to note that the Explorer pane may show workspaces that you don't have access to, but only the items you have permission to view will be displayed in the data hub."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> The article discusses two ways to endorse valuable items in Microsoft Fabric: promotion and certification. Promotion is used to highlight items that are considered valuable and encourages their collaborative use within an organization. Any item owner or user with write permissions can promote an item. Certification, on the other hand, indicates that an item meets the organization's quality standards and is reliable and authoritative. Only authorized reviewers can certify items. The article provides instructions on how to promote items and how to certify them if you are an authorized reviewer. It also explains how to request certification if you are not authorized. It is important to note that currently, Power BI dashboards cannot be endorsed."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> navigate to the settings of the item.\n",
       "\n",
       "2. Expand the endorsement section and select Request certification.\n",
       "\n",
       "3. Fill out the required information in the certification request form.\n",
       "\n",
       "4. Submit the request.\n",
       "\n",
       "Once the request is submitted, it will be reviewed by authorized users who can certify\n",
       "\n",
       "items. If the item meets the certification standards, it will be certified. If not, the\n",
       "\n",
       "request may be denied or further action may be required.\n",
       "\n",
       "In summary, to promote content, go to the settings of the content and select the\n",
       "\n",
       "\"Promoted\" option. To certify an item, review it, go to the item's settings, select\n",
       "\n",
       "\"Certified,\" and apply the changes. If you don't have authorization to certify an item,\n",
       "\n",
       "you can request certification by navigating to the item's settings, selecting \"Request\n",
       "\n",
       "certification,\" and filling out the required information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> This document provides information on how to apply sensitivity labels to Microsoft Fabric items. Sensitivity labels from Microsoft Purview Information Protection can help protect sensitive content and ensure compliance with governance requirements. By correctly labeling data with sensitivity labels, only authorized individuals can access the data. The document also mentions that Microsoft Fabric is currently in preview and may undergo substantial modifications before its release. Additionally, it provides a note about applying sensitivity labels in Power BI Desktop and lists the prerequisites for applying sensitivity labels to Microsoft Fabric items."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> To apply sensitivity labels to Fabric items, you will need a Power BI Pro or Premium Per User (PPU) license and edit permissions on the item you want to label. If you are unable to apply a sensitivity label or if the label is greyed out in the sensitivity label menu, it may be due to a lack of permissions. In such cases, it is recommended to contact your organization's tech support.\n",
       "\n",
       "There are two main ways to apply a sensitivity label to an item: through the flyout menu in the item header or in the item settings. To use the flyout menu, select the sensitivity indication in the header, which will display the flyout menu. Alternatively, you can open the item's settings, locate the sensitivity section, and choose the desired label from there.\n",
       "\n",
       "For more information on sensitivity labels, you can refer to the sensitivity label overview."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = summary['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
