{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering and building Cache/KB\n",
    "This notebook shows the pattern to persist the information from the question answering system to a cache or KB. This is useful when the question answering system is slow and we want to avoid calling it for every question. The cache can be built offline and then used for answering questions. This notebook shows how to build the cache and how to use it for answering questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in openAiEndPoint.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = openAiEndPoint\n",
    "davincimodel = OpenAiDavinci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch, generateEmbeddings, performCogVectorSearch, performKbCogVectorSearch, indexDocs\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"https://{OpenAiService}.openai.azure.com\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(model=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "# The simplest way of using Langchain and LLM is to use load_qa_chain and run it with a query and a list of documents.\n",
    "overrideChain = \"stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (overrideChain == \"stuff\"):\n",
    "    template = \"\"\"\n",
    "    Given the following extracted parts of a long document and a question, create a final answer. \n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "    If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "    QUESTION: {question}\n",
    "    =========\n",
    "    {summaries}\n",
    "    =========\n",
    "    \"\"\"\n",
    "    #qaPrompt = load_prompt('lc://prompts/qa_with_sources/stuff/basic.json')\n",
    "    qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "    #qaChain = load_qa_chain(llm, chain_type=overrideChain, prompt=qaPrompt)\n",
    "    qaChain = load_qa_with_sources_chain(llm, chain_type=overrideChain, prompt=qaPrompt)\n",
    "\n",
    "    # followupTemplate = \"\"\"\n",
    "    # Perform the following steps in a consecutive order Step 1, Step 2, Step 3, and Step 4. \n",
    "    # Step 1 Generate 10 questions based on the {context}?. \n",
    "    # Step 2 – Generate 5 more questions about \"{context}\" that do not repeat the above. \n",
    "    # Step 3 – Generate 5 more questions about \"{context}\" that do not repeat the above. \n",
    "    # Step 4 – Based on the above Steps 1,2,3 suggest a final list of questions avoiding duplicates or \n",
    "    # semantically similar questions.\n",
    "    # Use double angle brackets to reference the questions, e.g. <>.\n",
    "    # ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "    # \"\"\"\n",
    "    followupTemplate = \"\"\"\n",
    "    Generate three very brief follow-up questions that the user would likely ask next.\n",
    "    Use double angle brackets to reference the questions, e.g. <>.\n",
    "    Try not to repeat questions that have already been asked.\n",
    "\n",
    "    Return the questions in the following format:\n",
    "    <>\n",
    "    <>\n",
    "    <>\n",
    "\n",
    "    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "\n",
    "    =========\n",
    "    {context}\n",
    "    =========\n",
    "\n",
    "    \"\"\"\n",
    "    followupPrompt = PromptTemplate(template=followupTemplate, input_variables=[\"context\"])\n",
    "    followupChain = load_qa_chain(llm, chain_type=overrideChain, prompt=followupPrompt)\n",
    "elif (overrideChain == \"map_rerank\"):\n",
    "    outputParser = RegexParser(\n",
    "        regex=r\"(.*?)\\nScore: (.*)\",\n",
    "        output_keys=[\"answer\", \"score\"],\n",
    "    )\n",
    "\n",
    "    promptTemplate = \"\"\"\n",
    "    \n",
    "    Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
    "\n",
    "    Question: [question here]\n",
    "    [answer here]\n",
    "    Score: [score between 0 and 100]\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Context:\n",
    "    ---------\n",
    "    {summaries}\n",
    "    ---------\n",
    "    Question: {question}\n",
    "\n",
    "    \"\"\"\n",
    "    qaPrompt = PromptTemplate(template=promptTemplate,input_variables=[\"summaries\", \"question\"],\n",
    "                                output_parser=outputParser)\n",
    "    qaChain = load_qa_with_sources_chain(llm, chain_type=chainType,\n",
    "                                prompt=qaPrompt)\n",
    "\n",
    "    followupTemplate = \"\"\"\n",
    "    Generate three very brief follow-up questions that the user would likely ask next.\n",
    "    Use double angle brackets to reference the questions, e.g. <>.\n",
    "    Try not to repeat questions that have already been asked.\n",
    "\n",
    "    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "\n",
    "    =========\n",
    "    {context}\n",
    "    =========\n",
    "\n",
    "    \"\"\"\n",
    "    followupPrompt = PromptTemplate(template=followupTemplate, input_variables=[\"context\"])\n",
    "    followupChain = load_qa_chain(llm, chain_type='stuff', prompt=followupPrompt)\n",
    "elif (overrideChain == \"map_reduce\"):\n",
    "\n",
    "    qaTemplate = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "    Return any relevant text.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Relevant text, if any :\"\"\"\n",
    "\n",
    "    qaPrompt = PromptTemplate(\n",
    "        template=qaTemplate, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    combinePromptTemplate = \"\"\"\n",
    "        Given the following extracted parts of a long document and a question, create a final answer. \n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "        If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "        QUESTION: {question}\n",
    "        =========\n",
    "        {summaries}\n",
    "        =========\n",
    "        \"\"\"\n",
    "    combinePrompt = PromptTemplate(\n",
    "        template=combinePromptTemplate, input_variables=[\"summaries\", \"question\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    #qaChain = load_qa_chain(llm, chain_type=overrideChain, question_prompt=qaPrompt, combine_prompt=combinePrompt)\n",
    "    qaChain = load_qa_with_sources_chain(llm, chain_type=overrideChain, question_prompt=qaPrompt, combine_prompt=combinePrompt)\n",
    "    \n",
    "    followupTemplate = \"\"\"\n",
    "    Generate three very brief follow-up questions that the user would likely ask next.\n",
    "    Use double angle brackets to reference the questions, e.g. <>.\n",
    "    Try not to repeat questions that have already been asked.\n",
    "\n",
    "    Return the questions in the following format:\n",
    "    <>\n",
    "    <>\n",
    "    <>\n",
    "\n",
    "    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "\n",
    "    =========\n",
    "    {context}\n",
    "    =========\n",
    "\n",
    "    \"\"\"\n",
    "    followupPrompt = PromptTemplate(template=followupTemplate, input_variables=[\"context\"])\n",
    "    followupChain = load_qa_chain(llm, chain_type='stuff', prompt=followupPrompt)\n",
    "elif (overrideChain == \"refine\"):\n",
    "    refineTemplate = (\n",
    "        \"The original question is as follows: {question}\\n\"\n",
    "        \"We have provided an existing answer, including sources: {existing_answer}\\n\"\n",
    "        \"We have the opportunity to refine the existing answer\"\n",
    "        \"(only if needed) with some more context below.\\n\"\n",
    "        \"------------\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"------------\\n\"\n",
    "        \"Given the new context, refine the original answer to better \"\n",
    "        \"If you do update it, please update the sources as well. \"\n",
    "        \"If the context isn't useful, return the original answer.\"\n",
    "    )\n",
    "    refinePrompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "        template=refineTemplate,\n",
    "    )\n",
    "\n",
    "    qaTemplate = \"\"\"\n",
    "        Given the following extracted parts of a long document and a question, create a final answer. \n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "        If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "        QUESTION: {question}\n",
    "        =========\n",
    "        {context_str}\n",
    "        =========\n",
    "        \"\"\"\n",
    "    qaPrompt = PromptTemplate(\n",
    "        input_variables=[\"context_str\", \"question\"], template=qaTemplate\n",
    "    )\n",
    "    qaChain = load_qa_with_sources_chain(llm, chain_type=overrideChain, question_prompt=qaPrompt, refine_prompt=refinePrompt)\n",
    "\n",
    "    \n",
    "    followupTemplate = \"\"\"\n",
    "    Generate three very brief follow-up questions that the user would likely ask next.\n",
    "    Use double angle brackets to reference the questions, e.g. <>.\n",
    "    Try not to repeat questions that have already been asked.\n",
    "\n",
    "    Return the questions in the following format:\n",
    "    <>\n",
    "    <>\n",
    "    <>\n",
    "    \n",
    "    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "\n",
    "    =========\n",
    "    {context}\n",
    "    =========\n",
    "\n",
    "    \"\"\"\n",
    "    followupPrompt = PromptTemplate(template=followupTemplate, input_variables=[\"context\"])\n",
    "    followupChain = load_qa_chain(llm, chain_type='stuff', prompt=followupPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index aoaikb already exists\n"
     ]
    }
   ],
   "source": [
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "topK = 3\n",
    "question = \"What is ADF\"\n",
    "vectorQuestion = generateEmbeddings(OpenAiService, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, question)\n",
    "indexType = 'cogsearchvs'\n",
    "kbIndexName = 'aoaikb'\n",
    "\n",
    "# Let's perform the search on the KB first before asking the question to the model\n",
    "kbSearch = performKbCogVectorSearch(vectorQuestion, 'vectorQuestion', SearchService, SearchKey, indexType, indexName, kbIndexName, 1, [\"id\", \"question\", \"indexType\", \"indexName\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(kbSearch.get_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callLlm():\n",
    "    import uuid\n",
    "\n",
    "    # Call LLM to answer the question\n",
    "    r = performCogVectorSearch(vectorQuestion, 'contentVector', SearchService, SearchKey, indexName, topK)\n",
    "    if r.get_count() == 0:\n",
    "        docs = [Document(page_content=\"No results found\")]\n",
    "    else :\n",
    "        docs = [\n",
    "            Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "            for doc in r\n",
    "            ]\n",
    "    rawDocs=[]\n",
    "    for doc in docs:\n",
    "        rawDocs.append(doc.page_content)\n",
    "\n",
    "    answer = qaChain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n",
    "    answer = answer['output_text'].replace(\"Answer: \", '').replace(\"Sources:\", 'SOURCES:').replace(\"Next Questions:\", 'NEXT QUESTIONS:')\n",
    "    modifiedAnswer = answer\n",
    "\n",
    "    if overrideChain == \"stuff\" or overrideChain == \"map_rerank\":\n",
    "        thoughtPrompt = qaPrompt.format(question=question, summaries=rawDocs)\n",
    "    elif overrideChain == \"map_reduce\":\n",
    "        thoughtPrompt = qaPrompt.format(question=question, context=rawDocs)\n",
    "    elif overrideChain == \"refine\":\n",
    "        thoughtPrompt = qaPrompt.format(question=question, context_str=rawDocs)\n",
    "\n",
    "\n",
    "    # Followup questions\n",
    "    followupAnswer = followupChain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n",
    "    nextQuestions = followupAnswer['output_text'].replace(\"Answer: \", '').replace(\"Sources:\", 'SOURCES:').replace(\"Next Questions:\", 'NEXT QUESTIONS:').replace('NEXT QUESTIONS:', '').replace('NEXT QUESTIONS', '')\n",
    "    sources = ''                \n",
    "    if (modifiedAnswer.find(\"I don't know\") >= 0):\n",
    "        sources = ''\n",
    "        nextQuestions = ''\n",
    "    else:\n",
    "        sources = sources + \"\\n\" + docs[0].metadata['source']\n",
    "\n",
    "    outputData = []\n",
    "    outputData.append({\"data_points\": rawDocs, \"answer\": modifiedAnswer, \n",
    "            \"thoughts\": f\"<br><br>Prompt:<br>\" + thoughtPrompt.replace('\\n', '<br>'),\n",
    "                \"sources\": sources, \"nextQuestions\": nextQuestions, \"error\": \"\"})\n",
    "    \n",
    "    # Now that we got the answer, let's save it to the KB\n",
    "\n",
    "    kbData = []\n",
    "    id = str(uuid.uuid4())\n",
    "    kbData.append({\n",
    "        \"id\": id,\n",
    "        \"question\": question,\n",
    "        \"indexType\": indexType,\n",
    "        \"indexName\": indexName,\n",
    "        \"vectorQuestion\": vectorQuestion,\n",
    "        \"answer\": str({\"data_points\": rawDocs, \"answer\": modifiedAnswer, \n",
    "            \"thoughts\": f\"<br><br>Prompt:<br>\" + thoughtPrompt.replace('\\n', '<br>'),\n",
    "                \"sources\": sources, \"nextQuestions\": nextQuestions, \"error\": \"\"}),\n",
    "    })\n",
    "\n",
    "    indexDocs(SearchService, SearchKey, kbIndexName, kbData)\n",
    "    print(outputData)\n",
    "    print(vectorQuestion)\n",
    "    print(kbData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_points': [\"Fabric allows creators to concentrate on producing their best work, freeing them from\\n\\nthe need to integrate, manage, or understand the underlying infrastructure that\\n\\nsupports the experience.\\n\\nComponents of Microsoft Fabric\\n\\nMicrosoft Fabric offers the comprehensive set of analytics experiences designed to work\\n\\ntogether seamlessly. Each experience is tailored to a specific persona and a specific task.\\n\\nFabric includes industry-leading experiences in the following categories for an end-to-\\n\\nend analytical need.\\n\\nData Engineering - Data Engineering experience provides a world class Spark\\n\\nplatform with great authoring experiences, enabling data engineers to perform\\n\\nlarge scale data transformation and democratize data through the lakehouse.\\n\\nMicrosoft Fabric Spark's integration with Data Factory enables notebooks and\\n\\nspark jobs to be scheduled and orchestrated. For more information, see What is\\n\\nData engineering in Microsoft Fabric?\\n\\n\\x0cData Factory - Azure Data Factory combines the simplicity of Power Query with the\\n\\nscale and power of Azure Data Factory. You can use more than 200 native\\n\\nconnectors to connect to data sources on-premises and in the cloud. For more\\n\\ninformation, see What is Data Factory in Microsoft Fabric?\\n\\nData Science - Data Science experience enables you to build, deploy, and\\n\\noperationalize machine learning models seamlessly within your Fabric experience.\\n\\nIt integrates with Azure Machine Learning to provide built-in experiment tracking\", \"Tell us about your PDF experience.\\n\\nMicrosoft Fabric get started\\ndocumentation\\n\\nMicrosoft Fabric is a unified platform that can meet your organization's data and\\nanalytics needs. Discover the Fabric shared and platform documentation from this page.\\n\\nAbout Microsoft Fabric\\n\\nｅ OVERVIEW\\n\\nWhat is Fabric?\\n\\nFabric terminology\\n\\nｂ GET STARTED\\n\\nStart a Fabric trial\\n\\nFabric home navigation\\n\\nEnd-to-end tutorials\\n\\nContext sensitive Help pane\\n\\nGet started with Fabric items\\n\\nｐ CONCEPT\\n\\nFind items in OneLake data hub\\n\\nPromote and certify items\\n\\nｃ HOW-TO GUIDE\\n\\nApply sensitivity labels\\n\\nWorkspaces\\n\\nｐ CONCEPT\\n\\nFabric workspace\\n\\n\\x0cWorkspace roles\\n\\nｂ GET STARTED\\n\\nCreate a workspace\\n\\nｃ HOW-TO GUIDE\\n\\nWorkspace access control\\n\\n\\x0cWhat is Microsoft Fabric?\\n\\nArticle • 05/23/2023\\n\\nMicrosoft Fabric is an all-in-one analytics solution for enterprises that covers everything\\n\\nfrom data movement to data science, Real-Time Analytics, and business intelligence. It\\n\\noffers a comprehensive suite of services, including data lake, data engineering, and data\\n\\nintegration, all in one place.\\n\\nWith Fabric, you don't need to piece together different services from multiple vendors.\\n\\nInstead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is\\n\\ndesigned to simplify your analytics needs.\\n\\nThe platform is built on a foundation of Software as a Service (SaaS), which takes\\n\\nsimplicity and integration to a whole new level.\\n\\n） Important\", \"Data Factory\\n\\nIn this tutorial, you ingest data with data pipelines and transform data with\\ndataflows, then use the automation and notification to create a complete data\\nintegration scenario.\\n\\nData Science\\nend-to-end AI\\n\\nIn this set of tutorials, learn about the different Data Science experience\\ncapabilities and examples of how ML models can address your common\\n\\nsamples\\n\\nbusiness problems.\\n\\nData Science -\\nPrice prediction\\nwith R\\n\\nIn this tutorial, you build a machine learning model to analyze and visualize the\\navocado prices in the US and predict future prices.\\n\\nNext steps\\n\\nCreate a workspace\\n\\nDiscover data items in the OneLake data hub\\n\\n\\x0cMicrosoft Fabric decision guide: copy\\nactivity, dataflow, or Spark\\n\\nArticle • 05/23/2023\\n\\nUse this reference guide and the example scenarios to help you in deciding whether you\\n\\nneed a copy activity, a dataflow, or Spark for your workloads using Microsoft Fabric.\\n\\n） Important\\n\\nMicrosoft Fabric is currently in PREVIEW. This information relates to a prerelease\\n\\nproduct that may be substantially modified before it's released. Microsoft makes no\\n\\nwarranties, expressed or implied, with respect to the information provided here.\\n\\nCopy activity, dataflow, and Spark properties\\n\\nUse case\\n\\nPrimary\\ndeveloper\\n\\npersona\\n\\nPrimary\\ndeveloper skill\\n\\nset\\n\\nCode written\\n\\nPipeline copy activity\\n\\nData lake and data warehouse\\nmigration, \\ndata ingestion, \\nlightweight transformation\\n\\nData engineer, \\ndata integrator\\n\\nETL, \\nSQL, \\n\\nJSON\\n\\nNo code, \\nlow code\"], 'answer': 'Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. Data Factory is a part of Microsoft Fabric that combines the simplicity of Power Query with the scale and power of Azure Data Factory. It provides a world-class Spark platform with great authoring experiences, enabling data engineers to perform large scale data transformation and democratize data through the lakehouse.', 'thoughts': '<br><br>Prompt:<br><br>    Given the following extracted parts of a long document and a question, create a final answer. <br>    If you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer. <br>    If the answer is not contained within the text below, say \"I don\\'t know\".<br><br>    QUESTION: What is ADF<br>    =========<br>    [\"Fabric allows creators to concentrate on producing their best work, freeing them from\\\\n\\\\nthe need to integrate, manage, or understand the underlying infrastructure that\\\\n\\\\nsupports the experience.\\\\n\\\\nComponents of Microsoft Fabric\\\\n\\\\nMicrosoft Fabric offers the comprehensive set of analytics experiences designed to work\\\\n\\\\ntogether seamlessly. Each experience is tailored to a specific persona and a specific task.\\\\n\\\\nFabric includes industry-leading experiences in the following categories for an end-to-\\\\n\\\\nend analytical need.\\\\n\\\\nData Engineering - Data Engineering experience provides a world class Spark\\\\n\\\\nplatform with great authoring experiences, enabling data engineers to perform\\\\n\\\\nlarge scale data transformation and democratize data through the lakehouse.\\\\n\\\\nMicrosoft Fabric Spark\\'s integration with Data Factory enables notebooks and\\\\n\\\\nspark jobs to be scheduled and orchestrated. For more information, see What is\\\\n\\\\nData engineering in Microsoft Fabric?\\\\n\\\\n\\\\x0cData Factory - Azure Data Factory combines the simplicity of Power Query with the\\\\n\\\\nscale and power of Azure Data Factory. You can use more than 200 native\\\\n\\\\nconnectors to connect to data sources on-premises and in the cloud. For more\\\\n\\\\ninformation, see What is Data Factory in Microsoft Fabric?\\\\n\\\\nData Science - Data Science experience enables you to build, deploy, and\\\\n\\\\noperationalize machine learning models seamlessly within your Fabric experience.\\\\n\\\\nIt integrates with Azure Machine Learning to provide built-in experiment tracking\", \"Tell us about your PDF experience.\\\\n\\\\nMicrosoft Fabric get started\\\\ndocumentation\\\\n\\\\nMicrosoft Fabric is a unified platform that can meet your organization\\'s data and\\\\nanalytics needs. Discover the Fabric shared and platform documentation from this page.\\\\n\\\\nAbout Microsoft Fabric\\\\n\\\\nｅ OVERVIEW\\\\n\\\\nWhat is Fabric?\\\\n\\\\nFabric terminology\\\\n\\\\nｂ GET STARTED\\\\n\\\\nStart a Fabric trial\\\\n\\\\nFabric home navigation\\\\n\\\\nEnd-to-end tutorials\\\\n\\\\nContext sensitive Help pane\\\\n\\\\nGet started with Fabric items\\\\n\\\\nｐ CONCEPT\\\\n\\\\nFind items in OneLake data hub\\\\n\\\\nPromote and certify items\\\\n\\\\nｃ HOW-TO GUIDE\\\\n\\\\nApply sensitivity labels\\\\n\\\\nWorkspaces\\\\n\\\\nｐ CONCEPT\\\\n\\\\nFabric workspace\\\\n\\\\n\\\\x0cWorkspace roles\\\\n\\\\nｂ GET STARTED\\\\n\\\\nCreate a workspace\\\\n\\\\nｃ HOW-TO GUIDE\\\\n\\\\nWorkspace access control\\\\n\\\\n\\\\x0cWhat is Microsoft Fabric?\\\\n\\\\nArticle • 05/23/2023\\\\n\\\\nMicrosoft Fabric is an all-in-one analytics solution for enterprises that covers everything\\\\n\\\\nfrom data movement to data science, Real-Time Analytics, and business intelligence. It\\\\n\\\\noffers a comprehensive suite of services, including data lake, data engineering, and data\\\\n\\\\nintegration, all in one place.\\\\n\\\\nWith Fabric, you don\\'t need to piece together different services from multiple vendors.\\\\n\\\\nInstead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is\\\\n\\\\ndesigned to simplify your analytics needs.\\\\n\\\\nThe platform is built on a foundation of Software as a Service (SaaS), which takes\\\\n\\\\nsimplicity and integration to a whole new level.\\\\n\\\\n） Important\", \"Data Factory\\\\n\\\\nIn this tutorial, you ingest data with data pipelines and transform data with\\\\ndataflows, then use the automation and notification to create a complete data\\\\nintegration scenario.\\\\n\\\\nData Science\\\\nend-to-end AI\\\\n\\\\nIn this set of tutorials, learn about the different Data Science experience\\\\ncapabilities and examples of how ML models can address your common\\\\n\\\\nsamples\\\\n\\\\nbusiness problems.\\\\n\\\\nData Science -\\\\nPrice prediction\\\\nwith R\\\\n\\\\nIn this tutorial, you build a machine learning model to analyze and visualize the\\\\navocado prices in the US and predict future prices.\\\\n\\\\nNext steps\\\\n\\\\nCreate a workspace\\\\n\\\\nDiscover data items in the OneLake data hub\\\\n\\\\n\\\\x0cMicrosoft Fabric decision guide: copy\\\\nactivity, dataflow, or Spark\\\\n\\\\nArticle • 05/23/2023\\\\n\\\\nUse this reference guide and the example scenarios to help you in deciding whether you\\\\n\\\\nneed a copy activity, a dataflow, or Spark for your workloads using Microsoft Fabric.\\\\n\\\\n） Important\\\\n\\\\nMicrosoft Fabric is currently in PREVIEW. This information relates to a prerelease\\\\n\\\\nproduct that may be substantially modified before it\\'s released. Microsoft makes no\\\\n\\\\nwarranties, expressed or implied, with respect to the information provided here.\\\\n\\\\nCopy activity, dataflow, and Spark properties\\\\n\\\\nUse case\\\\n\\\\nPrimary\\\\ndeveloper\\\\n\\\\npersona\\\\n\\\\nPrimary\\\\ndeveloper skill\\\\n\\\\nset\\\\n\\\\nCode written\\\\n\\\\nPipeline copy activity\\\\n\\\\nData lake and data warehouse\\\\nmigration, \\\\ndata ingestion, \\\\nlightweight transformation\\\\n\\\\nData engineer, \\\\ndata integrator\\\\n\\\\nETL, \\\\nSQL, \\\\n\\\\nJSON\\\\n\\\\nNo code, \\\\nlow code\"]<br>    =========<br>    ', 'sources': '\\nFabric Get Started.pdf', 'nextQuestions': 'What are the benefits of using Microsoft Fabric for data analytics?\\n    How does Microsoft Fabric simplify the analytics process?\\n    Can you explain the difference between copy activity, dataflow, and Spark in Microsoft Fabric? \\n', 'error': ''}\n"
     ]
    }
   ],
   "source": [
    "if kbSearch.get_count() > 0:\n",
    "    for s in kbSearch:\n",
    "        if s['@search.score'] >= 0.95:\n",
    "            print(s['answer'])\n",
    "        else:\n",
    "            callLlm()\n",
    "else:\n",
    "    callLlm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
